# scripts-jiu/process_subtitle_optimized.py
import os
import re
import json
import argparse
import time
import sys
from typing import List, Dict, Tuple
from pathlib import Path
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==================== é…ç½® ====================
LLM_API = "http://localhost:8001/v1/completions"
CONFIG_PATH = r"D:\AI\CineGraph-AI\config\mashup_optimized_config.json"

# ä¼˜åŒ–åçš„æ ‡ç­¾ä½“ç³» - å‡å°‘é‡å¤ï¼Œå¢åŠ å¤šæ ·æ€§
DEFAULT_MASHUP_CONFIG = {
    "version": "v4.0-mashup-optimized",
    
    # æ ¸å¿ƒåŠŸèƒ½æ ‡ç­¾ï¼ˆä¸€çº§æ ‡ç­¾ï¼Œæ§åˆ¶ä½¿ç”¨é¢‘ç‡ï¼‰
    "primary_functions": [
        "å¼ºè¡Œè§£é‡Š", "èº«ä»½åè½¬", "åœºæ™¯å«æ¥", "é‡‘å¥å¼•ç”¨", 
        "è·¨æœèŠå¤©", "åå·®èŒ", "ä¸€æœ¬æ­£ç»èƒ¡è¯´", "é™ç»´æ‰“å‡»",
        "æ—¶ä»£é”™ä½", "æ¬¡å…ƒçªç ´", "ç¥è½¬æŠ˜", "åºŸè¯æ–‡å­¦"
    ],
    
    # é£æ ¼/æ•ˆæœæ ‡ç­¾ï¼ˆäºŒçº§æ ‡ç­¾ï¼Œå¢åŠ ç»†åˆ†ï¼‰
    "style_effects": [
        "åè®½é«˜çº§é»‘", "è‡ªå˜²è§£æ„", "è°éŸ³æ¢—ç‹", "åŒå…³å¤§å¸ˆ",
        "å¤¸å¼ æ¯”å–»", "æ­£è¯åè¯´", "æ— æ•ˆæ²Ÿé€š", "èœœæ±è‡ªä¿¡",
        "å¼±å°å¯æ€œ", "åš£å¼ è·‹æ‰ˆ", "å‚²å¨‡å£å«Œ", "å‡¡å°”èµ›æ–‡å­¦"
    ],
    
    # è¿æ¥æ–¹å¼ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
    "connection_types": [
        "æ¥åè½¬", "æ¥è´¨ç–‘", "æ¥è‡ªå˜²", "æ¥ç©æ¢—",
        "æ¥å†·åœº", "æ¥çˆ†å‘", "æ¥è§£é‡Š", "æ¥åæ§½",
        "æ¥æ±‚é¥¶", "æ¥å‚²å¨‡", "æ¥è£…å‚»", "æ¥æš´æ€’"
    ],
    
    # å‰ªè¾‘èŠ‚å¥ï¼ˆæ ‡å‡†åŒ–ï¼‰
    "editing_rhythms": [
        "å¿«é€Ÿåˆ‡æ¢—", "æ…¢æ”¾æ‰“è„¸", "é‡å¤é¬¼ç•œ", "æˆ›ç„¶è€Œæ­¢",
        "é€’è¿›å¤¸å¼ ", "çªç„¶æ‰“æ–­", "ç”»å¤–éŸ³æ€¼", "ç”»é¢ç¥é…"
    ],
    
    # è·¨ç•Œç±»å‹ï¼ˆé˜²æ­¢é‡å¤ç»„åˆï¼‰
    "crossover_genres": [
        {"type": "å¤è£…+ç§‘å¹»", "example": "ã€Šç”„å¬›ä¼ ã€‹+ã€Šæ˜Ÿé™…ç©¿è¶Šã€‹"},
        {"type": "åŠ¨ç”»+ç°å®", "example": "ã€Šæµ·ç»µå®å®ã€‹+èŒåœºå‰§"},
        {"type": "æ­¦ä¾ +ç°ä»£", "example": "ã€Šç¬‘å‚²æ±Ÿæ¹–ã€‹+åŠå…¬å®¤"},
        {"type": "ææ€–+å–œå‰§", "example": "ã€Šå’’æ€¨ã€‹+ã€Šå®¶æœ‰å„¿å¥³ã€‹"},
        {"type": "æ—¥æ¼«+å›½å‰§", "example": "ã€Šç«å½±å¿è€…ã€‹+ã€Šè¿˜ç æ ¼æ ¼ã€‹"},
        {"type": "æ¬§ç¾+å¤é£", "example": "ã€ŠæƒåŠ›çš„æ¸¸æˆã€‹+ã€Šä¸‰å›½æ¼”ä¹‰ã€‹"}
    ],
    
    # éŸ³æ•ˆåº“ï¼ˆä¸°å¯ŒéŸ³æ•ˆå»ºè®®ï¼‰
    "sound_effects": [
        "å˜é€Ÿå¤„ç†", "å›å£°æ•ˆæœ", "æ··å“å¤„ç†", "ç”µå­å˜å£°",
        "ç¯å¢ƒéŸ³çªæ˜¾", "BGMéª¤åœ", "éŸ³æ•ˆå åŠ ", "é™éŸ³åå·®"
    ]
}

def load_mashup_config() -> dict:
    config = DEFAULT_MASHUP_CONFIG.copy()
    if os.path.exists(CONFIG_PATH):
        try:
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                user_config = json.load(f)
            for key in config.keys():
                if key in user_config and user_config[key]:
                    config[key] = user_config[key]
            print(f"âœ… ä¼˜åŒ–é…ç½®åŠ è½½æˆåŠŸ: ç‰ˆæœ¬ {config['version']}")
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
    return config

MASHUP_CONFIG = load_mashup_config()
CONFIG_VERSION = MASHUP_CONFIG["version"]

# ==================== ä¼˜åŒ–æç¤ºè¯ ====================
def build_optimized_prompt(current_line: str, context_lines: List[str]) -> str:
    """
    ä¼˜åŒ–ç‰ˆæ··å‰ªæç¤ºè¯ - å¼ºè°ƒå¤šæ ·æ€§å’Œå®ç”¨æ€§
    """
    
    primary_funcs = ", ".join(MASHUP_CONFIG["primary_functions"])
    style_effects = ", ".join(MASHUP_CONFIG["style_effects"])
    connections = ", ".join(MASHUP_CONFIG["connection_types"])
    rhythms = ", ".join(MASHUP_CONFIG["editing_rhythms"])
    sound_effects = ", ".join(MASHUP_CONFIG["sound_effects"])
    
    # éšæœºé€‰æ‹©è·¨ç•Œç±»å‹ç¤ºä¾‹ï¼ˆé¿å…æ¯æ¬¡ç›¸åŒï¼‰
    import random
    crossover_samples = random.sample(MASHUP_CONFIG["crossover_genres"], 3)
    crossover_examples = "\n".join([f"- {item['type']}: {item['example']}" for item in crossover_samples])
    
    return f"""
### ğŸ¬ æ··å‰ªåˆ›ä½œæ½œåŠ›åˆ†æ - ä¼˜åŒ–ç‰ˆ ###

## ğŸ“‹ ä½ çš„è§’è‰²
ä½ æ˜¯èµ„æ·±å½±è§†æ··å‰ªUPä¸»ï¼Œæ“…é•¿åˆ¶ä½œè·¨ä½œå“ã€æ— å˜å¤´ã€æç¬‘å‘çš„æ··å‰ªè§†é¢‘ã€‚

## ğŸ¯ æ ¸å¿ƒä»»åŠ¡
åˆ†æä»¥ä¸‹å°è¯åœ¨**è„±ç¦»åŸç‰‡è¯­å¢ƒ**åçš„æ··å‰ªæ½œåŠ›ã€‚å¿˜è®°å°è¯åœ¨åŸç‰‡ä¸­çš„æ„æ€ï¼Œä¸“æ³¨äºï¼š
1. åœ¨å…¶ä»–ä½œå“ä¸­èƒ½äº§ç”Ÿä»€ä¹ˆæç¬‘æ•ˆæœï¼Ÿ
2. é€‚åˆç”¨ä»€ä¹ˆå‰ªè¾‘æ‰‹æ³•ï¼Ÿ
3. èƒ½å’Œä»€ä¹ˆç±»å‹çš„ä½œå“/å°è¯æ‹¼æ¥ï¼Ÿ

## âš ï¸ å¤šæ ·æ€§è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰
1. **é¿å…æ ‡ç­¾å †ç Œ**ï¼šä¸è¦è¿‡åº¦ä½¿ç”¨"å¼ºè¡Œè§£é‡Š"å’Œ"é™æ™ºæ‰“å‡»"
2. **åˆ›æ„å¤šæ ·åŒ–**ï¼šæä¾›2ç§ä¸åŒé£æ ¼çš„åˆ›æ„ç¤ºä¾‹ï¼Œé¿å…é‡å¤ç»„åˆ
3. **è¿æ¥å¤šæ ·åŒ–**ï¼šæ ¹æ®å°è¯ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„è¿æ¥æ–¹å¼ï¼Œä¸åªæ˜¯"æ¥æ±‚é¥¶"
4. **éŸ³æ•ˆå¤šæ ·åŒ–**ï¼šé™¤äº†"å˜é€Ÿå¤„ç†"ï¼Œæä¾›æ›´å¤šéŸ³æ•ˆå»ºè®®

## ğŸ“ åˆ†æå¯¹è±¡
å½“å‰å°è¯: "{current_line}"
ä¸Šä¸‹æ–‡: {json.dumps(context_lines, ensure_ascii=False)[:150]}...

## ğŸ·ï¸ æ ‡ç­¾åº“å‚è€ƒ
æ ¸å¿ƒåŠŸèƒ½: {primary_funcs}
é£æ ¼æ•ˆæœ: {style_effects}
è¿æ¥æ–¹å¼: {connections}
å‰ªè¾‘èŠ‚å¥: {rhythms}
éŸ³æ•ˆå»ºè®®: {sound_effects}

è·¨ç•Œç±»å‹ç¤ºä¾‹:
{crossover_examples}

## ğŸ“Š è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰
{{
  "mashup_analysis": {{
    // æ ¸å¿ƒæ ‡ç­¾ï¼ˆç²¾ç®€ï¼Œç”¨äºå¿«é€Ÿç­›é€‰ï¼‰
    "quick_tags": {{
      "primary": "ä¸»è¦åŠŸèƒ½ï¼ˆä»æ ¸å¿ƒåŠŸèƒ½ä¸­é€‰æ‹©ï¼Œé¿å…é‡å¤ä½¿ç”¨ï¼‰",
      "style": "é£æ ¼æ•ˆæœï¼ˆä»é£æ ¼æ•ˆæœä¸­é€‰æ‹©ï¼Œç²¾å‡†åŒ¹é…ï¼‰",
      "connection": "è¿æ¥æ–¹å¼ï¼ˆä»è¿æ¥æ–¹å¼ä¸­é€‰æ‹©ï¼Œå¤šæ ·åŒ–ï¼‰",
      "rhythm": "å‰ªè¾‘èŠ‚å¥ï¼ˆä»å‰ªè¾‘èŠ‚å¥ä¸­é€‰æ‹©ï¼‰"
    }},
    
    // è¯­ä¹‰æ‘˜è¦ï¼ˆç”¨äºå‘é‡åŒ–æœç´¢ï¼‰
    "semantic_summary": {{
      "brief": "ä¸€å¥ç®€çŸ­æè¿°ï¼ˆ20å­—ä»¥å†…ï¼‰",
      "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
      "humor_style": "å¹½é»˜é£æ ¼æè¿°",
      "use_case": "é€‚ç”¨åœºæ™¯æè¿°"
    }},
    
    // åˆ›æ„å‚æ•°ï¼ˆç”¨äºå‚è€ƒï¼‰
    "creative_params": {{
      "crossover_types": ["è·¨ç•Œç±»å‹1", "è·¨ç•Œç±»å‹2"],
      "audio_suggestions": ["éŸ³æ•ˆå»ºè®®1", "éŸ³æ•ˆå»ºè®®2"],
      "visual_suggestions": ["ç”»é¢å»ºè®®1", "ç”»é¢å»ºè®®2"]
    }},
    
    // åˆ›æ„ç¤ºä¾‹ï¼ˆå¤šæ ·åŒ–ï¼Œæ¯ä¸ªç¤ºä¾‹50å­—ä»¥å†…ï¼‰
    "creative_examples": [
      {{
        "style": "åˆ›æ„é£æ ¼",
        "description": "åˆ›æ„æè¿°ï¼ˆé¿å…é‡å¤IPç»„åˆï¼‰",
        "key_elements": ["å…ƒç´ 1", "å…ƒç´ 2"]
      }},
      {{
        "style": "å¦ä¸€ç§é£æ ¼",
        "description": "å¦ä¸€ç§åˆ›æ„æè¿°",
        "key_elements": ["ä¸åŒå…ƒç´ 1", "ä¸åŒå…ƒç´ 2"]
      }}
    ],
    
    // å®ç”¨æŠ€å·§ï¼ˆå…·ä½“å¯æ“ä½œï¼‰
    "practical_tips": [
      "å‰ªè¾‘æŠ€å·§1ï¼ˆå…·ä½“ï¼‰",
      "éŸ³æ•ˆæŠ€å·§1ï¼ˆå¤šæ ·ï¼‰",
      "èŠ‚å¥å»ºè®®ï¼ˆæœ‰åˆ›æ„ï¼‰"
    ]
  }}
}}

## âœ¨ ä¼˜è´¨åˆ†ææ ‡å‡†
1. **æ ‡ç­¾ç²¾å‡†**ï¼šé€‰æ‹©æœ€è´´åˆçš„æ ‡ç­¾ï¼Œä¸å †ç Œ
2. **åˆ›æ„å¤šæ ·**ï¼šæä¾›2ç§ä¸åŒé£æ ¼çš„åˆ›æ„
3. **å»ºè®®å®ç”¨**ï¼šç»™å‡ºå…·ä½“å¯æ“ä½œçš„å‰ªè¾‘å»ºè®®
4. **é¿å…é‡å¤**ï¼šä¸ä½¿ç”¨æœ€è¿‘åˆ†æä¸­å¸¸ç”¨çš„IPç»„åˆ

ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚
"""

# ==================== åå¤„ç†å‡½æ•° ====================
def post_process_annotation(annotation_data: Dict) -> Dict:
    """
    åå¤„ç†å‡½æ•°ï¼šå¢å¼ºæ ‡æ³¨æ•°æ®çš„å¤šæ ·æ€§å’Œå®ç”¨æ€§
    """
    mashup = annotation_data.get("mashup_analysis", {})
    
    # 1. ç¡®ä¿è¯­ä¹‰æ‘˜è¦ç®€æ´ï¼ˆä¾¿äºå‘é‡åŒ–ï¼‰
    if "semantic_summary" in mashup:
        semantic = mashup["semantic_summary"]
        # ç¡®ä¿briefä¸è¶…è¿‡30å­—
        if "brief" in semantic and len(semantic["brief"]) > 30:
            semantic["brief"] = semantic["brief"][:30] + "..."
        # ç¡®ä¿keywordsä¸è¶…è¿‡5ä¸ª
        if "keywords" in semantic and len(semantic["keywords"]) > 5:
            semantic["keywords"] = semantic["keywords"][:5]
    
    # 2. ç¡®ä¿åˆ›æ„ç¤ºä¾‹ç®€æ´
    if "creative_examples" in mashup:
        # æ¯ä¸ªç¤ºä¾‹é™åˆ¶é•¿åº¦
        for example in mashup["creative_examples"]:
            if "description" in example and len(example["description"]) > 60:
                example["description"] = example["description"][:60] + "..."
            # ç¡®ä¿key_elementsä¸è¶…è¿‡3ä¸ª
            if "key_elements" in example and len(example["key_elements"]) > 3:
                example["key_elements"] = example["key_elements"][:3]
    
    # 3. ç¡®ä¿å®ç”¨æŠ€å·§å…·ä½“
    if "practical_tips" in mashup:
        # é™åˆ¶æŠ€å·§æ•°é‡
        if len(mashup["practical_tips"]) > 3:
            mashup["practical_tips"] = mashup["practical_tips"][:3]
    
    # 4. ç”Ÿæˆå‘é‡åŒ–å‹å¥½å­—æ®µ
    annotation_data["vector_friendly"] = {
        "primary_tag": mashup.get("quick_tags", {}).get("primary", ""),
        "style_tag": mashup.get("quick_tags", {}).get("style", ""),
        "connection_tag": mashup.get("quick_tags", {}).get("connection", ""),
        "keywords": mashup.get("semantic_summary", {}).get("keywords", []),
        "humor_style": mashup.get("semantic_summary", {}).get("humor_style", ""),
        "use_case": mashup.get("semantic_summary", {}).get("use_case", "")
    }
    
    # 5. ç”Ÿæˆç”¨äºæœç´¢çš„æ ‡ç­¾å­—ç¬¦ä¸²
    quick_tags = mashup.get("quick_tags", {})
    semantic = mashup.get("semantic_summary", {})
    
    annotation_data["search_tags"] = [
        quick_tags.get("primary", ""),
        quick_tags.get("style", ""),
        quick_tags.get("connection", ""),
        quick_tags.get("rhythm", "")
    ] + semantic.get("keywords", [])
    
    # è¿‡æ»¤ç©ºå€¼
    annotation_data["search_tags"] = [tag for tag in annotation_data["search_tags"] if tag]
    
    return annotation_data

# ==================== å­—å¹•è§£æ ====================
def parse_srt(file_path: str) -> List[Dict]:
    """è§£æSRTå­—å¹•æ–‡ä»¶"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        content = f.read()
    
    blocks = re.split(r"\n\s*\n", content.strip())
    lines = []
    for block in blocks:
        parts = block.strip().split("\n")
        if len(parts) < 3: continue
        try:
            time_range = parts[1]
            text = " ".join(parts[2:]).replace("\n", " ").strip()
            if not text or "-->" not in time_range: continue
            start_str, end_str = time_range.split(" --> ")
            lines.append({
                "text": text,
                "start": _time_to_seconds(start_str),
                "end": _time_to_seconds(end_str)
            })
        except: continue
    return lines

def _time_to_seconds(time_str: str) -> float:
    h, m, s_ms = time_str.replace(",", ".").split(":")
    return float(h) * 3600 + float(m) * 60 + float(s_ms)

# ==================== JSONè§£æ ====================
def parse_llm_response(response_text: str) -> Dict:
    """è§£æLLMå“åº”"""
    # å¼ºåŠ›å‰¥ç¦»Markdownä»£ç å—
    clean_text = re.sub(r'```(?:json)?|```', '', response_text).strip()
    
    # å°è¯•å¯»æ‰¾JSONè¾¹ç•Œ
    start_idx = clean_text.find('{')
    end_idx = clean_text.rfind('}')
    if start_idx != -1 and end_idx != -1:
        clean_text = clean_text[start_idx:end_idx+1]
    
    try:
        parsed = json.loads(clean_text)
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "mashup_analysis" not in parsed:
            raise ValueError("Missing mashup_analysis")
        
        # åº”ç”¨åå¤„ç†
        parsed = post_process_annotation(parsed)
        
        return parsed
        
    except Exception as e:
        print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
        # è¿”å›é»˜è®¤ç»“æ„
        return create_default_annotation()

def create_default_annotation() -> Dict:
    """åˆ›å»ºé»˜è®¤æ ‡æ³¨"""
    return {
        "mashup_analysis": {
            "quick_tags": {
                "primary": "å…¶ä»–",
                "style": "å…¶ä»–",
                "connection": "æ¥å¸¸è§„",
                "rhythm": "å¸¸è§„å‰ªè¾‘"
            },
            "semantic_summary": {
                "brief": "æ— æ˜æ˜¾ç‰¹å¾",
                "keywords": ["å¸¸è§„"],
                "humor_style": "æ— ",
                "use_case": "é€šç”¨åœºæ™¯"
            },
            "creative_params": {
                "crossover_types": ["é€šç”¨"],
                "audio_suggestions": ["å¸¸è§„éŸ³æ•ˆ"],
                "visual_suggestions": ["å¸¸è§„ç”»é¢"]
            },
            "creative_examples": [
                {
                    "style": "é€šç”¨",
                    "description": "å¸¸è§„æ··å‰ªåº”ç”¨",
                    "key_elements": ["é€šç”¨å…ƒç´ "]
                }
            ],
            "practical_tips": ["æŒ‰å¸¸è§„å‰ªè¾‘å³å¯"]
        },
        "vector_friendly": {
            "primary_tag": "å…¶ä»–",
            "style_tag": "å…¶ä»–",
            "connection_tag": "æ¥å¸¸è§„",
            "keywords": ["å¸¸è§„"],
            "humor_style": "æ— ",
            "use_case": "é€šç”¨åœºæ™¯"
        },
        "search_tags": ["å…¶ä»–", "å¸¸è§„"]
    }

# ==================== è¯­ä¹‰æ ‡æ³¨ ====================
def annotate_line(current_line: str, context_lines: List[str], max_retries=2) -> Dict:
    """æ ‡æ³¨å•è¡Œå°è¯"""
    prompt = build_optimized_prompt(current_line, context_lines)
    
    # ä¿®æ”¹APIè·¯å¾„
    CHAT_API = LLM_API.replace("/completions", "/chat/completions")
    
    for attempt in range(max_retries):
        try:
            response = requests.post(
                CHAT_API,
                json={
                    "model": "qwen3-chat",
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯æ··å‰ªåˆ›ä½œä¸“å®¶ï¼Œåˆ†æè¦ç²¾å‡†ã€æœ‰åˆ›æ„ã€å¤šæ ·åŒ–ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.7,  # æé«˜åˆ›é€ æ€§
                    "response_format": {"type": "json_object"},
                    "max_tokens": 2000
                },
                timeout=30
            )
            response.raise_for_status()
            res_json = response.json()
            
            # é€‚é…ä¸åŒAPIè¿”å›ç»“æ„
            if "choices" in res_json and "message" in res_json["choices"][0]:
                content = res_json["choices"][0]["message"]["content"].strip()
            elif "choices" in res_json and "text" in res_json["choices"][0]:
                content = res_json["choices"][0]["text"].strip()
            else:
                raise ValueError(f"æ— æ³•è¯†åˆ«çš„APIè¿”å›ç»“æ„: {res_json}")
            
            # è§£æå“åº”
            result = parse_llm_response(content)
            result["config_version"] = CONFIG_VERSION
            result["analysis_time"] = time.time()
            
            return result
            
        except Exception as e:
            print(f"âŒ ç¬¬ {attempt+1} æ¬¡å°è¯•å¤±è´¥: {e}")
            if attempt == max_retries - 1:
                result = create_default_annotation()
                result["config_version"] = CONFIG_VERSION
                result["analysis_time"] = time.time()
                return result
            time.sleep(1)

# ==================== å•è¡Œå¤„ç†å‡½æ•° ====================
def process_single_line(line_data, idx, total, window_size, all_lines):
    """å¤„ç†å•è¡Œå­—å¹•"""
    start_idx = max(0, idx - window_size)
    end_idx = min(total, idx + window_size + 1)
    context_texts = [all_lines[j]["text"] for j in range(start_idx, end_idx)]
    
    result = annotate_line(line_data["text"], context_texts)
    
    return {
        "id": f"line_{idx}",
        "text": line_data["text"],
        "start": line_data["start"],
        "end": line_data["end"],
        
        # æ··å‰ªåˆ†æç»“æœ
        **result,
        
        # ä¸Šä¸‹æ–‡ä¿¡æ¯
        "context": {
            "previous": context_texts[0] if len(context_texts) > 1 else "",
            "next": context_texts[-1] if len(context_texts) > 1 else ""
        }
    }

# ==================== ä¸»å¤„ç†æµç¨‹ ====================
def process_srt_file(input_path: str, output_dir: str, window_size: int = 2, max_workers: int = 4):
    """ä¸»å¤„ç†å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¬ å½±è§†æ··å‰ªè¯­ä¹‰æ ‡æ³¨å·¥å…· - ä¼˜åŒ–ç‰ˆ v4.0")
    print("ğŸ¯ ç‰¹æ€§ï¼šå¤šæ ·æ€§ä¼˜åŒ– + å‘é‡åŒ–å‹å¥½")
    print("=" * 60)
    
    print(f"ğŸ” å¯åŠ¨åˆ†æ: {input_path}")
    print(f"âš™ï¸ é…ç½®ç‰ˆæœ¬: {CONFIG_VERSION}")
    
    # è§£æå­—å¹•
    lines = parse_srt(input_path)
    if not lines:
        print("âŒ æœªè§£æåˆ°æœ‰æ•ˆå­—å¹•å†…å®¹")
        return
    
    total = len(lines)
    print(f"ğŸ“Š æ‰¾åˆ° {total} è¡Œå­—å¹•")
    
    annotated_lines = [None] * total
    start_time = time.time()
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_single_line, lines[i], i, total, window_size, lines): i
            for i in range(total)
        }
        
        completed = 0
        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                annotated_lines[idx] = result
                completed += 1
                
                # è¿›åº¦æ˜¾ç¤º
                progress_interval = max(1, total // 10)
                if completed % max(1, progress_interval) == 0:
                    elapsed = time.time() - start_time
                    speed = completed / elapsed if elapsed > 0 else 0
                    remaining = (total - completed) / speed if speed > 0 else 0
                    
                    print(f"ğŸ”„ è¿›åº¦: {completed}/{total} ({completed/total:.1%}) | "
                          f"é€Ÿåº¦: {speed:.1f}è¡Œ/ç§’ | å‰©ä½™: {remaining:.0f}ç§’")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªç¤ºä¾‹
                if completed == 1:
                    print(f"\nğŸ“ ç¤ºä¾‹æ ‡æ³¨ç»“æœ:")
                    print(f"   å°è¯: {result['text'][:50]}...")
                    print(f"   æ ¸å¿ƒæ ‡ç­¾: {result['mashup_analysis']['quick_tags']['primary']}")
                    print(f"   è¿æ¥æ–¹å¼: {result['mashup_analysis']['quick_tags']['connection']}")
                    print(f"   è¯­ä¹‰æ‘˜è¦: {result['mashup_analysis']['semantic_summary']['brief']}")
                    print()
                    
            except Exception as e:
                print(f"âŒ è¡Œ {idx} å¤„ç†å¤±è´¥: {e}")
                annotated_lines[idx] = create_default_line_result(idx, lines[idx] if idx < len(lines) else None)
    
    # ç§»é™¤å¯èƒ½çš„Noneå€¼
    annotated_lines = [line for line in annotated_lines if line is not None]
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    output_path = Path(output_dir) / f"{Path(input_path).stem}_optimized.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(annotated_lines, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ç®€åŒ–ç‰ˆï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
    simple_data = create_simple_version(annotated_lines)
    simple_path = Path(output_dir) / f"{Path(input_path).stem}_simple.json"
    with open(simple_path, "w", encoding="utf-8") as f:
        json.dump(simple_data, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = generate_statistics(annotated_lines)
    stats_path = Path(output_dir) / f"{Path(input_path).stem}_stats.json"
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nâœ¨ å¤„ç†å®Œæˆï¼")
    print(f"â±ï¸ æ€»è€—æ—¶: {time.time() - start_time:.1f}ç§’")
    print(f"ğŸ“ˆ å¤„ç†äº† {len(annotated_lines)} è¡Œå­—å¹•")
    print(f"ğŸ’¾ å®Œæ•´ç»“æœ: {output_path}")
    print(f"ğŸ“‹ ç®€åŒ–ç‰ˆæœ¬: {simple_path}")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
    
    # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
    print(f"\nğŸ­ æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
    for tag_type, counts in stats.get("tag_distribution", {}).items():
        print(f"  {tag_type}:")
        for tag, count in list(counts.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            percentage = count / len(annotated_lines) * 100
            print(f"    {tag}: {count} è¡Œ ({percentage:.1f}%)")

def create_default_line_result(idx: int, line_data: Dict) -> Dict:
    """åˆ›å»ºé»˜è®¤è¡Œç»“æœ"""
    if line_data:
        text = line_data["text"]
        start = line_data["start"]
        end = line_data["end"]
    else:
        text = ""
        start = 0
        end = 0
    
    default_annotation = create_default_annotation()
    
    return {
        "id": f"line_{idx}",
        "text": text,
        "start": start,
        "end": end,
        **default_annotation,
        "config_version": CONFIG_VERSION,
        "analysis_time": time.time(),
        "context": {"previous": "", "next": ""}
    }

def create_simple_version(annotated_lines: List[Dict]) -> List[Dict]:
    """åˆ›å»ºç®€åŒ–ç‰ˆæœ¬"""
    simple_lines = []
    for line in annotated_lines:
        simple_lines.append({
            "id": line["id"],
            "text": line["text"][:60] + ("..." if len(line["text"]) > 60 else ""),
            "primary_tag": line["mashup_analysis"]["quick_tags"]["primary"],
            "style_tag": line["mashup_analysis"]["quick_tags"]["style"],
            "connection_tag": line["mashup_analysis"]["quick_tags"]["connection"],
            "brief": line["mashup_analysis"]["semantic_summary"]["brief"],
            "keywords": line["mashup_analysis"]["semantic_summary"]["keywords"],
            "humor_style": line["mashup_analysis"]["semantic_summary"]["humor_style"]
        })
    return simple_lines

def generate_statistics(annotated_lines: List[Dict]) -> Dict:
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    tag_distribution = {
        "primary": {},
        "style": {},
        "connection": {},
        "rhythm": {}
    }
    
    crossover_counts = {}
    humor_style_counts = {}
    
    for line in annotated_lines:
        mashup = line["mashup_analysis"]
        
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        quick_tags = mashup["quick_tags"]
        for tag_type in tag_distribution.keys():
            tag = quick_tags.get(tag_type, "")
            tag_distribution[tag_type][tag] = tag_distribution[tag_type].get(tag, 0) + 1
        
        # ç»Ÿè®¡è·¨ç•Œç±»å‹
        crossover_types = mashup["creative_params"].get("crossover_types", [])
        for ct in crossover_types:
            crossover_counts[ct] = crossover_counts.get(ct, 0) + 1
        
        # ç»Ÿè®¡å¹½é»˜é£æ ¼
        humor_style = mashup["semantic_summary"].get("humor_style", "")
        if humor_style:
            humor_style_counts[humor_style] = humor_style_counts.get(humor_style, 0) + 1
    
    return {
        "total_lines": len(annotated_lines),
        "config_version": CONFIG_VERSION,
        "tag_distribution": tag_distribution,
        "crossover_distribution": dict(sorted(crossover_counts.items(), key=lambda x: x[1], reverse=True)[:10]),
        "humor_style_distribution": dict(sorted(humor_style_counts.items(), key=lambda x: x[1], reverse=True)[:10]),
        "processing_timestamp": time.time()
    }

# ==================== CLI ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å½±è§†å°è¯è¯­ä¹‰æ ‡æ³¨å·¥å…·-ä¼˜åŒ–ç‰ˆ v4.0")
    parser.add_argument("input", help="SRTæ–‡ä»¶è·¯å¾„")
    parser.add_argument("output_dir", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--window", type=int, default=2, help="ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆå‰åå¥æ•°ï¼‰")
    parser.add_argument("--workers", type=int, default=4, help="å¹¶å‘å¤„ç†çº¿ç¨‹æ•°")
    
    args = parser.parse_args()
    
    try:
        process_srt_file(args.input, args.output_dir, args.window, args.workers)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)