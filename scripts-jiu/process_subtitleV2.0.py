# scripts-jiu/process_subtitle.py
import os
import re
import json
import argparse
import time
import sys
from typing import List, Dict
from pathlib import Path
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==================== é…ç½® ====================
LLM_API = "http://localhost:8001/v1/completions"
CONFIG_PATH = r"D:\AI\CineGraph-AI\config\theme_config.json"

# é»˜è®¤é…ç½®
DEFAULT_CONFIG = {
    "version": "v1.1-mashup-optimized",
    "emotions": ["å–œæ‚¦", "æ„¤æ€’", "æ‚²ä¼¤", "ææƒ§", "æƒŠè®¶", "è®½åˆº", "å¹½é»˜", "ä¸­æ€§", "å°´å°¬", "åš£å¼ "],
    "themes": [
        "èº«ä»½é”™ä½", "è¯­è¨€è’è¯", "é€»è¾‘å¼ºè½¬", "ä¸‡èƒ½è¡”æ¥", 
        "å‘èµ·æé—®", "æ‹’ç»é‚€çº¦", "é“å¾·ç»‘æ¶", "å‡¡å°”èµ›", "æ‰“è„¸", "å…¶ä»–"
    ],
    "priority_emotions": ["è®½åˆº", "å¹½é»˜", "åš£å¼ "] 
}



def load_semantic_config() -> dict:
    config = DEFAULT_CONFIG.copy()
    if os.path.exists(CONFIG_PATH):
        try:
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                user_config = json.load(f)
            for key in ["version", "emotions", "themes", "priority_emotions"]:
                if key in user_config and user_config[key]:
                    config[key] = user_config[key]
            print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: ç‰ˆæœ¬ {config['version']}")
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
    return config

SEMANTIC_CONFIG = load_semantic_config()
EMOTIONS = SEMANTIC_CONFIG["emotions"]
THEMES = SEMANTIC_CONFIG["themes"]
PRIORITY_EMOTIONS = SEMANTIC_CONFIG["priority_emotions"]
CONFIG_VERSION = SEMANTIC_CONFIG["version"]

# ==================== ä¼˜åŒ–åçš„æ ¸å¿ƒæç¤ºè¯ ====================
def build_prompt(current_line: str, context_lines: List[str]) -> str:
    """
    å½±è§†æ··å‰ªè¯­ä¹‰æ ‡æ³¨æç¤ºè¯ - ä¸¥æ ¼æ¸…æ´ç‰ˆ
    è§£å†³æ¨¡å‹å¤è¯»å ä½ç¬¦ã€å…¬å¼ç¬¦å·çš„é—®é¢˜ã€‚
    """
    # åŠ¨ä½œæ„å›¾åº“
    ACTION_INTENT_TAGS = [
        "ä¸€æœ¬æ­£ç»èƒ¡è¯´å…«é“", "é™æ™ºæ‰“å‡»", "é˜´é˜³æ€ªæ°”", "å¼ºè¡Œç¼åˆ", "å€Ÿæœºå‘éš¾", 
        "è£…ç–¯å–å‚»", "æ°”åœºå‹åˆ¶", "è·¨ç•Œå€Ÿæ¢—", "é“å¾·ç»‘æ¶", "å¼ºè¡Œç‹¡è¾©",
        "ç¡®è®¤èº«ä»½", "æ¨å¸è´£ä»»", "å§”å©‰æ‹’ç»", "å‘å‡ºå¨èƒ", "å‘èµ·æé—®"
    ]
    
    # åŠŸèƒ½æ ‡ç­¾åº“
    MASHUP_FUNCTIONAL_TAGS = [
        "åç›´è§‰è½¬æŠ˜", "é€»è¾‘é¬¼æ‰", "æ¬¡å…ƒå£ç¢°æ’", "ä¸‡èƒ½è¡”æ¥", "æƒ…ç»ªçˆ†å‘", 
        "é€»è¾‘æ–­å±‚", "æŒ‡ä»£ä¸æ˜", "æ‰“ç ´ç¬¬å››é¢å¢™", "æ— æ•ˆæ²Ÿé€š", "æ°›å›´çƒ˜æ‰˜",
        "åŠ¨ä½œè¡”æ¥", "é‡‘å¥ç»“æŸ", "ä¸‡èƒ½è½¬åœº", "ä¸‡èƒ½å¼€åœº", "èº«ä»½ç¡®è®¤"
    ]

    action_tags_str = ", ".join(ACTION_INTENT_TAGS)
    function_tags_str = ", ".join(MASHUP_FUNCTIONAL_TAGS)
    
    return (
        "### Role ###\n"
        "ä½ æ˜¯ä¸€ä½æ·±è°™å½±è§†è§£æ„æ–‡åŒ–çš„å‰ªè¾‘ä¸“å®¶ã€‚ä½ éœ€è¦å°†å°è¯è½¬åŒ–ä¸ºé«˜ä»·å€¼çš„è¯­ä¹‰æ‘˜è¦ã€‚\n\n"

        "### å†™ä½œç¦ä»¤ (å¿…é¡»ä¸¥æ ¼éµå®ˆ) ###\n"
        "1. **ä¸¥ç¦è¾“å‡ºå ä½ç¬¦**ï¼šç¦æ­¢åœ¨ subtext ä¸­å‡ºç° '{ }'ã€'['ã€']' ä»¥å¤–çš„å¼•å¯¼ç¬¦å·ï¼Œç¦æ­¢å‡ºç° '+' å·ã€‚\n"
        "2. **ä¸¥ç¦å¤è¯»å¼•å¯¼è¯**ï¼šç¦æ­¢åœ¨ subtext ä¸­å‡ºç° 'å¯é€‰'ã€'æ ‡ç­¾åº“'ã€'å°è¯é‡Œçš„äº‹' ç­‰æç¤ºè¯é‡Œçš„è¯æ±‡ã€‚\n"
        "3. **è‡ªç„¶è¯­è¨€åŒ–**ï¼šsubtext å¿…é¡»æ˜¯ä¸€ä¸ªæµç•…çš„å¥å­ï¼Œè€Œä¸æ˜¯æ ‡ç­¾çš„ç®€å•å †ç Œã€‚\n\n"

        "### subtext ç¼–å†™æ ‡å‡† ###\n"
        "è¯·æŒ‰ç…§ä»¥ä¸‹é€»è¾‘æ’°å†™ï¼š\n"
        "1. å¥é¦–å¿…é¡»æ˜¯ä¸€ä¸ª [åŠ¨ä½œæ„å›¾æ ‡ç­¾]ã€‚\n"
        "2. ç´§æ¥ç€ç”¨ä¸€æ®µå…·ä½“çš„æ–‡å­—æè¿°å°è¯é‡Œçš„å…·ä½“äººç‰©ã€åŠ¨ä½œæˆ–ç‰©å“ã€‚ä¸è¦æŠ½è±¡ï¼Œè¦å…·ä½“ã€‚\n"
        "3. å¥å°¾å¿…é¡»æ˜¯ä¸€ä¸ª [æ··å‰ªåŠŸèƒ½æ ‡ç­¾]ã€‚\n"
        "ç¤ºä¾‹æ ¼å¼ï¼š[æ°”åœºå‹åˆ¶] è§’è‰²å¯¹ç€é‚£ç®±é‡‘å­éœ²å‡ºè´ªå©ªçš„ç›®å…‰å¹¶ä»¥æ­¤å¨èƒå¯¹æ–¹ï¼Œ[ä¸‡èƒ½è½¬åœº]\n\n"

        "### æ ‡ç­¾åº“ (è¯·ä»ä¸­æŒ‘é€‰) ###\n"
        f"- åŠ¨ä½œæ„å›¾: {action_tags_str}\n"
        f"- æ··å‰ªåŠŸèƒ½: {function_tags_str}\n\n"

        "### JSON Output Format ###\n"
        "{\n"
        '  "line_annotation": {"emotion": "...", "theme": "...", "subtext": "..."},\n'
        '  "context_annotation": {"emotion": "...", "theme": "...", "subtext": "..."},\n'
        f'  "config_version": "{CONFIG_VERSION}"\n'
        "}\n\n"
        
        "### å¾…åˆ†ææ•°æ® ###\n"
        f"å°è¯å†…å®¹: '{current_line}'\n"
        f"ä¸Šä¸‹æ–‡è¯­å¢ƒ: {json.dumps(context_lines, ensure_ascii=False)}\n\n"
        
        "### ä»»åŠ¡æ‰§è¡Œ ###\n"
        "ç°åœ¨è¯·ç›´æ¥è¾“å‡ºè¯¥å°è¯çš„ JSON æ ‡æ³¨ç»“æœã€‚subtext ä¸­å¿…é¡»ç”¨å…·ä½“å°è¯å†…å®¹æ›¿æ¢æ‰æ‰€æœ‰çš„æè¿°æ€§å ä½ç¬¦ã€‚"
    )
# ==================== å­—å¹•è§£æ ====================
def parse_srt(file_path: str) -> List[Dict]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        content = f.read()
    
    blocks = re.split(r"\n\s*\n", content.strip())
    lines = []
    for block in blocks:
        parts = block.strip().split("\n")
        if len(parts) < 3: continue
        try:
            time_range = parts[1]
            text = " ".join(parts[2:]).replace("\n", " ").strip()
            if not text or "-->" not in time_range: continue
            start_str, end_str = time_range.split(" --> ")
            lines.append({
                "text": text,
                "start": _time_to_seconds(start_str),
                "end": _time_to_seconds(end_str)
            })
        except: continue
    return lines

def _time_to_seconds(time_str: str) -> float:
    h, m, s_ms = time_str.replace(",", ".").split(":")
    return float(h) * 3600 + float(m) * 60 + float(s_ms)

# ==================== JSONè§£æä¸éªŒè¯ ====================
def parse_llm_response(response_text: str) -> dict:
    # å¼ºåŠ›å‰¥ç¦» Markdown ä»£ç å—
    clean_text = re.sub(r'```(?:json)?|```', '', response_text).strip()
    
    # å°è¯•å¯»æ‰¾ JSON è¾¹ç•Œï¼ˆé˜²æ­¢å¼€å¤´æœ‰åºŸè¯ï¼‰
    start_idx = clean_text.find('{')
    end_idx = clean_text.rfind('}')
    if start_idx != -1 and end_idx != -1:
        clean_text = clean_text[start_idx:end_idx+1]

    try:
        parsed = json.loads(clean_text)
        # å­—æ®µè¡¥å…¨æ£€æµ‹
        if "line_annotation" not in parsed: raise ValueError("Missing line_annotation")
        return parsed
    except Exception:
        # æç®€å›é€€é€»è¾‘
        return {
            "line_annotation": {"emotion": "ä¸­æ€§", "theme": "å…¶ä»–", "subtext": "è¯­ä¹‰è§£æå›é€€ï¼šæ— æ˜æ˜¾ç‰¹å¾"},
            "context_annotation": {"emotion": "ä¸­æ€§", "theme": "å…¶ä»–", "subtext": "æ— æ˜ç¡®äº’åŠ¨æ„å›¾"},
            "config_version": CONFIG_VERSION
        }

# ==================== è¯­ä¹‰æ ‡æ³¨ ====================
def annotate_with_context(current_line: str, context_lines: List[str], max_retries=2) -> Dict:
    prompt = build_prompt(current_line, context_lines)
    
    # ä¿®æ”¹ API è·¯å¾„ä¸ºé€šç”¨çš„ Chat è·¯å¾„
    CHAT_API = LLM_API.replace("/completions", "/chat/completions")
    
    for attempt in range(max_retries):
        try:
            response = requests.post(
                CHAT_API,
                json={
                    "model": "qwen3-chat", 
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼åªè¾“å‡ºJSONçš„è¯­ä¹‰æ ‡æ³¨ä¸“å®¶ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.3,
                    "response_format": {"type": "json_object"}
                },
                timeout=30
            )
            response.raise_for_status()
            res_json = response.json()
            
            # é€‚é… Chat API çš„è¿”å›ç»“æ„
            if "choices" in res_json and "message" in res_json["choices"][0]:
                content = res_json["choices"][0]["message"]["content"].strip()
            elif "choices" in res_json and "text" in res_json["choices"][0]:
                content = res_json["choices"][0]["text"].strip()
            else:
                print(f"âš ï¸ æ— æ³•è¯†åˆ«çš„ API è¿”å›ç»“æ„: {res_json}")
                raise ValueError("Unknown API structure")

            # è°ƒè¯•æ‰“å°ï¼šå¦‚æœä½ åœ¨ç»ˆç«¯çœ‹åˆ°æ¨¡å‹è¿”å›çš„æ–‡å­—ï¼Œå°±çŸ¥é“å“ªé‡Œé”™äº†
            # print(f"DEBUG LLM è¿”å›å†…å®¹: {content}") 
            
            return parse_llm_response(content)
            
        except Exception as e:
            print(f"âŒ ç¬¬ {attempt+1} æ¬¡å°è¯•è¯·æ±‚å¤±è´¥: {e}")
            if attempt == max_retries - 1:
                return parse_llm_response("") 
            time.sleep(1)
# ==================== å•è¡Œå¤„ç†å‡½æ•° ====================
def process_single_line(line_data, idx, total, window_size, all_lines):
    start_idx = max(0, idx - window_size)
    end_idx = min(total, idx + window_size + 1)
    context_texts = [all_lines[j]["text"] for j in range(start_idx, end_idx)]
    
    result = annotate_with_context(line_data["text"], context_texts)
    
    return {
        "id": f"line_{idx}",
        "text": line_data["text"],
        "start": line_data["start"],
        "end": line_data["end"],
        "emotion": result["line_annotation"]["emotion"],
        "theme": result["line_annotation"]["theme"],
        "subtext": result["line_annotation"]["subtext"],
        "mashup_tags": {
            "context_emotion": result["context_annotation"]["emotion"],
            "context_theme": result["context_annotation"]["theme"],
            "context_subtext": result["context_annotation"]["subtext"]
        },
        "config_version": result.get("config_version", CONFIG_VERSION)
    }

# ==================== ä¸»å¤„ç†æµç¨‹ ====================
def process_srt_file(input_path: str, output_dir: str, window_size: int = 2, max_workers: int = 4):
    print(f"ğŸ” å¯åŠ¨è¯­ä¹‰åˆ†æ: {input_path}")
    lines = parse_srt(input_path)
    if not lines: return
    
    total = len(lines)
    annotated_lines = [None] * total
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_single_line, lines[i], i, total, window_size, lines): i
            for i in range(total)
        }
        
        completed = 0
        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                annotated_lines[idx] = result
                completed += 1
                if completed % max(1, total // 10) == 0:
                    speed = completed / (time.time() - start_time)
                    print(f"ğŸ”„ è¿›åº¦: {completed}/{total} ({completed/total:.1%}) | é€Ÿåº¦: {speed:.1f}è¡Œ/ç§’")
            except Exception as e:
                print(f"âŒ è¡Œ {idx} å¤„ç†å¤±è´¥: {e}")

    # ä¿å­˜æ–‡ä»¶
    output_path = Path(output_dir) / f"{Path(input_path).stem}_annotated.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(annotated_lines, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ¨ å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜è‡³: {output_path}")
    print(f"â±ï¸ æ€»è€—æ—¶: {time.time() - start_time:.1f}ç§’")

# ==================== CLI ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å½±è§†å°è¯è¯­ä¹‰æ ‡æ³¨å·¥å…·-æ··å‰ªå¢å¼ºç‰ˆ")
    parser.add_argument("input", help="SRTæ–‡ä»¶è·¯å¾„")
    parser.add_argument("output_dir", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--window", type=int, default=2, help="ä¸Šä¸‹æ–‡çª—å£")
    parser.add_argument("--workers", type=int, default=4, help="çº¿ç¨‹æ•°")
    
    args = parser.parse_args()
    process_srt_file(args.input, args.output_dir, args.window, args.workers)