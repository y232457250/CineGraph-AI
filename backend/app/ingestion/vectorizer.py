# backend/app/ingestion/vectorizer.py
"""
å‘é‡åŒ–å…¥åº“æ¨¡å— - å°†æ ‡æ³¨æ•°æ®å­˜å…¥ChromaDB
æ”¯æŒå•å¥å‘é‡åŒ–ï¼Œç”¨äºæ··å‰ªæ¥è¯æœç´¢
"""

import os
import json
import time
import yaml
import requests
from typing import List, Dict, Optional
from pathlib import Path
from dataclasses import dataclass

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("âš ï¸ ChromaDBæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install chromadb")


# ==================== é…ç½® ====================
BASE_DIR = Path(__file__).parent.parent.parent.parent
CONFIG_DIR = BASE_DIR / "config"
DATA_DIR = BASE_DIR / "data"
CHROMA_DB_PATH = DATA_DIR / "chroma_db"
EMBEDDING_CONFIG_PATH = CONFIG_DIR / "embedding_providers.yaml"


# ==================== Embeddingæä¾›è€… ====================
class EmbeddingProvider:
    """Embeddingæä¾›è€…"""
    
    def __init__(self, config: Dict):
        self.name = config.get("name", "Unknown")
        self.type = config.get("type", "local")
        self.base_url = config.get("base_url", "")
        self.model = config.get("model", "")
        self._has_dimension = "dimension" in config
        self.dimension = config.get("dimension", 1536)
        self.timeout = config.get("timeout", 60)
        self.api_style = config.get("api_style", "openai")
        self.truncate = config.get("truncate", True)
        
        # å¤„ç†API Key
        api_key = config.get("api_key", "")
        if api_key and api_key.startswith("${") and api_key.endswith("}"):
            env_var = api_key[2:-1]
            self.api_key = os.environ.get(env_var, "")
        else:
            self.api_key = api_key
    
    def embed(self, texts: List[str]) -> List[List[float]]:
        """è·å–æ–‡æœ¬å‘é‡"""
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        base_url = self.base_url.rstrip("/")
        if self.api_style == "ollama":
            url = f"{base_url}/embed" if base_url.endswith("/api") else f"{base_url}/api/embed"
            payload = {
                "model": self.model,
                "input": texts,
                "truncate": self.truncate
            }
            if self._has_dimension and self.dimension:
                payload["dimensions"] = self.dimension
        else:
            url = f"{base_url}/embeddings"
            payload = {
                "model": self.model,
                "input": texts
            }
        
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            result = response.json()
            
            # æå–embedding
            if "embeddings" in result:
                return result.get("embeddings", [])
            embeddings = []
            if "data" in result:
                for item in result["data"]:
                    embeddings.append(item["embedding"])
            return embeddings
        except Exception as e:
            raise Exception(f"Embeddingè°ƒç”¨å¤±è´¥: {e}")
    
    def embed_single(self, text: str) -> List[float]:
        """è·å–å•ä¸ªæ–‡æœ¬å‘é‡"""
        results = self.embed([text])
        return results[0] if results else []


class EmbeddingManager:
    """Embeddingç®¡ç†å™¨ - ä¼˜å…ˆä»æ•°æ®åº“è¯»å–ï¼Œå›é€€åˆ°YAML"""
    
    def __init__(self, config_path: Path = EMBEDDING_CONFIG_PATH):
        self.config_path = config_path
        self.providers: Dict[str, Dict] = {}
        self.active_provider: str = ""
        self._use_db = False
        self._load_config()
    
    def _load_config(self):
        """åŠ è½½é…ç½® - ä¼˜å…ˆæ•°æ®åº“ï¼Œå›é€€YAML"""
        try:
            self._load_from_db()
            if self.providers:
                self._use_db = True
                return
        except Exception as e:
            print(f"âš ï¸ ä»æ•°æ®åº“åŠ è½½Embeddingé…ç½®å¤±è´¥ï¼Œå›é€€åˆ°YAML: {e}")
        
        self._load_from_yaml()
    
    def _load_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½é…ç½®"""
        from app.core.model_provider_service import get_model_provider_service
        service = get_model_provider_service()
        
        providers_list = service.list_providers(category='embedding')
        if not providers_list:
            return
        
        self.providers = {}
        for p in providers_list:
            provider_id = p['id']
            config = service.get_provider_config(provider_id)
            if config:
                self.providers[provider_id] = config
                if p.get('is_active'):
                    self.active_provider = provider_id
        
        if not self.active_provider and self.providers:
            self.active_provider = next(iter(self.providers))
        
        print(f"âœ… Embeddingé…ç½®ä»æ•°æ®åº“åŠ è½½æˆåŠŸï¼Œå½“å‰ä½¿ç”¨: {self.active_provider} ({len(self.providers)} ä¸ªæä¾›è€…)")
    
    def _load_from_yaml(self):
        """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        if not self.config_path.exists():
            self._use_default_config()
            return
        
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            
            self.active_provider = config.get("active_provider", "local_qwen_embedding")
            
            for key, value in config.items():
                if isinstance(value, dict) and "base_url" in value:
                    self.providers[key] = value
            
            print(f"âœ… Embeddingé…ç½®ä»YAMLåŠ è½½æˆåŠŸï¼Œå½“å‰ä½¿ç”¨: {self.active_provider}")
        except Exception as e:
            print(f"âš ï¸ Embeddingé…ç½®åŠ è½½å¤±è´¥: {e}")
            self._use_default_config()
    
    def _use_default_config(self):
        """ä½¿ç”¨é»˜è®¤é…ç½®"""
        self.active_provider = "local_qwen_embedding"
        self.providers = {
            "local_qwen_embedding": {
                "name": "æœ¬åœ°Qwen3-Embedding",
                "type": "local",
                "base_url": "http://localhost:8002/v1",
                "model": "qwen3-embedding-4b",
                "dimension": 2560
            }
        }
    
    def get_provider(self, provider_name: str = None) -> EmbeddingProvider:
        """è·å–Embeddingæä¾›è€…"""
        name = provider_name or self.active_provider
        if name not in self.providers:
            raise ValueError(f"æœªçŸ¥çš„Embeddingæä¾›è€…: {name}")
        return EmbeddingProvider(self.providers[name])
    
    def get_dimension(self, provider_name: str = None) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        name = provider_name or self.active_provider
        if name in self.providers:
            return self.providers[name].get("dimension", 1536)
        return 1536


# ==================== ChromaDBç®¡ç† ====================
class VectorStore:
    """å‘é‡æ•°æ®åº“ç®¡ç†"""
    
    COLLECTION_NAME = "mashup_lines"
    
    def __init__(self, db_path: Path = CHROMA_DB_PATH):
        if not CHROMADB_AVAILABLE:
            raise ImportError("ChromaDBæœªå®‰è£…")
        
        self.db_path = db_path
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ChromaDB
        self.client = chromadb.PersistentClient(
            path=str(self.db_path),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # è·å–æˆ–åˆ›å»ºcollection
        self.collection = self.client.get_or_create_collection(
            name=self.COLLECTION_NAME,
            metadata={"description": "å°è¯æ··å‰ªå‘é‡åº“"}
        )
        
        print(f"âœ… ChromaDBåˆå§‹åŒ–æˆåŠŸï¼Œå½“å‰æœ‰ {self.collection.count()} æ¡è®°å½•")
    
    def add_lines(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict]
    ):
        """æ·»åŠ å°è¯å‘é‡"""
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )
        print(f"âœ… å·²æ·»åŠ  {len(ids)} æ¡è®°å½•")
    
    def upsert_lines(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict]
    ):
        """æ›´æ–°æˆ–æ’å…¥å°è¯å‘é‡"""
        self.collection.upsert(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )
        print(f"âœ… å·²æ›´æ–°/æ’å…¥ {len(ids)} æ¡è®°å½•")
    
    def search(
        self,
        query_embedding: List[float],
        n_results: int = 10,
        where: Dict = None,
        where_document: Dict = None
    ) -> Dict:
        """æœç´¢ç›¸ä¼¼å°è¯"""
        return self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where,
            where_document=where_document,
            include=["documents", "metadatas", "distances"]
        )
    
    def search_by_text(
        self,
        query_text: str,
        embedding_provider: EmbeddingProvider,
        n_results: int = 10,
        where: Dict = None
    ) -> List[Dict]:
        """é€šè¿‡æ–‡æœ¬æœç´¢ç›¸ä¼¼å°è¯"""
        # è·å–æŸ¥è¯¢å‘é‡
        query_embedding = embedding_provider.embed_single(query_text)
        
        # æœç´¢
        results = self.search(
            query_embedding=query_embedding,
            n_results=n_results,
            where=where
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted = []
        if results and results.get("ids"):
            ids = results["ids"][0]
            documents = results["documents"][0] if results.get("documents") else []
            metadatas = results["metadatas"][0] if results.get("metadatas") else []
            distances = results["distances"][0] if results.get("distances") else []
            
            for i, id_ in enumerate(ids):
                formatted.append({
                    "id": id_,
                    "text": documents[i] if i < len(documents) else "",
                    "metadata": metadatas[i] if i < len(metadatas) else {},
                    "distance": distances[i] if i < len(distances) else 0,
                    "score": 1 - (distances[i] if i < len(distances) else 0)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                })
        
        return formatted
    
    def search_next_line(
        self,
        current_line_id: str,
        embedding_provider: EmbeddingProvider,
        n_results: int = 10
    ) -> List[Dict]:
        """æœç´¢èƒ½æ¥çš„ä¸‹ä¸€å¥å°è¯"""
        # è·å–å½“å‰å°è¯ä¿¡æ¯
        current = self.collection.get(ids=[current_line_id], include=["metadatas"])
        
        if not current or not current.get("metadatas"):
            return []
        
        metadata = current["metadatas"][0]
        can_lead_to = metadata.get("can_lead_to", [])
        
        if not can_lead_to:
            # å¦‚æœæ²¡æœ‰can_lead_toï¼Œä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœç´¢
            return self.search_by_text(
                query_text=metadata.get("vector_text", ""),
                embedding_provider=embedding_provider,
                n_results=n_results
            )
        
        # æ ¹æ®can_lead_toè¿‡æ»¤
        # ChromaDBçš„whereæŸ¥è¯¢æ”¯æŒ$inæ“ä½œç¬¦
        results = []
        for sentence_type in can_lead_to:
            partial_results = self.search_by_text(
                query_text=metadata.get("vector_text", ""),
                embedding_provider=embedding_provider,
                n_results=n_results // len(can_lead_to) + 1,
                where={"sentence_type": sentence_type}
            )
            results.extend(partial_results)
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶å»é‡
        seen = set()
        unique_results = []
        for r in sorted(results, key=lambda x: x["score"], reverse=True):
            if r["id"] not in seen and r["id"] != current_line_id:
                seen.add(r["id"])
                unique_results.append(r)
        
        return unique_results[:n_results]
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        count = self.collection.count()
        
        # è·å–ç”µå½±åˆ†å¸ƒ
        all_data = self.collection.get(include=["metadatas"])
        movie_counts = {}
        sentence_type_counts = {}
        emotion_counts = {}
        
        if all_data and all_data.get("metadatas"):
            for meta in all_data["metadatas"]:
                # ç”µå½±ç»Ÿè®¡ - å…¼å®¹æ–°æ—§æ ¼å¼
                movie = meta.get("media_id") or meta.get("movie_title") or meta.get("movie_id") or meta.get("source_movie", "unknown")
                movie_counts[movie] = movie_counts.get(movie, 0) + 1
                
                # å¥å‹ç»Ÿè®¡
                st = meta.get("sentence_type", "unknown")
                sentence_type_counts[st] = sentence_type_counts.get(st, 0) + 1
                
                # æƒ…ç»ªç»Ÿè®¡
                emotion = meta.get("emotion", "unknown")
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        return {
            "total_lines": count,
            "movies": movie_counts,
            "sentence_types": sentence_type_counts,
            "emotions": emotion_counts
        }
    
    def delete_by_movie(self, movie_name: str):
        """åˆ é™¤æŸéƒ¨ç”µå½±çš„æ‰€æœ‰å°è¯"""
        # å°è¯•æ–°æ ¼å¼ (media_id)
        try:
            self.collection.delete(where={"media_id": movie_name})
        except:
            pass
        # å…¼å®¹æ—§æ ¼å¼
        try:
            self.collection.delete(where={"movie_id": movie_name})
        except:
            pass
        try:
            self.collection.delete(where={"source_movie": movie_name})
        except:
            pass
        print(f"âœ… å·²åˆ é™¤ç”µå½± '{movie_name}' çš„æ‰€æœ‰å°è¯")
    
    def reset(self):
        """é‡ç½®æ•°æ®åº“"""
        self.client.delete_collection(self.COLLECTION_NAME)
        self.collection = self.client.create_collection(
            name=self.COLLECTION_NAME,
            metadata={"description": "å°è¯æ··å‰ªå‘é‡åº“"}
        )
        print("âœ… å‘é‡åº“å·²é‡ç½®")


# ==================== å‘é‡åŒ–å™¨ ====================
class Vectorizer:
    """å‘é‡åŒ–å™¨ - å°†æ ‡æ³¨æ•°æ®å‘é‡åŒ–å¹¶å­˜å…¥ChromaDB"""
    
    def __init__(self, embedding_provider: str = None):
        self.embedding_manager = EmbeddingManager()
        
        if embedding_provider:
            self.embedding_manager.active_provider = embedding_provider
        
        self.embedding = self.embedding_manager.get_provider()
        self.store = VectorStore()
    
    def vectorize_annotations(
        self,
        annotations_path: str,
        batch_size: int = 50,
        progress_callback=None
    ) -> int:
        """å‘é‡åŒ–æ ‡æ³¨æ–‡ä»¶å¹¶å­˜å…¥æ•°æ®åº“"""
        
        # åŠ è½½æ ‡æ³¨æ•°æ®
        with open(annotations_path, "r", encoding="utf-8") as f:
            annotations = json.load(f)
        
        if not annotations:
            print("âŒ æ ‡æ³¨æ–‡ä»¶ä¸ºç©º")
            return 0
        
        total = len(annotations)
        print(f"ğŸ“Š å¼€å§‹å‘é‡åŒ– {total} æ¡æ ‡æ³¨...")
        
        start_time = time.time()
        processed = 0
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, total, batch_size):
            batch = annotations[i:i+batch_size]
            
            # å‡†å¤‡æ•°æ®
            ids = []
            texts = []
            documents = []
            metadatas = []
            
            for ann in batch:
                line_id = ann.get("id", f"line_{i}")
                
                # ä½¿ç”¨vector_textè¿›è¡Œå‘é‡åŒ–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨text
                vector_text = ann.get("vector_text", ann.get("text", ""))
                
                ids.append(line_id)
                texts.append(vector_text)
                documents.append(ann.get("text", ""))
                
                # è·å–åµŒå¥—çš„sourceå’Œmashup_tags
                source = ann.get("source", {})
                mashup_tags = ann.get("mashup_tags", {})
                editing_params = ann.get("editing_params", {})
                
                # å…¼å®¹æ—§æ ¼å¼ (source)
                if not source:
                    source = {
                        "media_id": ann.get("source_movie", "") or ann.get("movie_id", ""),
                        "start": ann.get("start", 0),
                        "end": ann.get("end", 0)
                    }
                
                # å…¼å®¹æ—§æ ¼å¼ (mashup_tags)
                if not mashup_tags:
                    mashup_tags = {
                        "sentence_type": ann.get("sentence_type", ""),
                        "emotion": ann.get("emotion", ""),
                        "tone": ann.get("tone", ""),
                        "character_type": ann.get("character_type", ""),
                        "can_follow": ann.get("can_follow", []),
                        "can_lead_to": ann.get("can_lead_to", []),
                        "keywords": ann.get("keywords", []),
                        "primary_function": ann.get("primary_function", ""),
                        "style_effect": ann.get("style_effect", "")
                    }
                
                # å…ƒæ•°æ® - ç²¾ç®€ç‰ˆ (ç”¨äºæœç´¢è¿‡æ»¤)
                metadatas.append({
                    "text": ann.get("text", ""),
                    # æ¥æºä¿¡æ¯ (ç²¾ç®€)
                    "media_id": source.get("media_id", "") or source.get("movie_id", ""),
                    "start": source.get("start", 0),
                    "end": source.get("end", 0),
                    # æ··å‰ªæ ‡ç­¾ (æ ¸å¿ƒ)
                    "sentence_type": mashup_tags.get("sentence_type", ""),
                    "emotion": mashup_tags.get("emotion", ""),
                    "tone": mashup_tags.get("tone", ""),
                    "character_type": mashup_tags.get("character_type", ""),
                    "can_follow": json.dumps(mashup_tags.get("can_follow", []), ensure_ascii=False),
                    "can_lead_to": json.dumps(mashup_tags.get("can_lead_to", []), ensure_ascii=False),
                    "keywords": json.dumps(mashup_tags.get("keywords", []), ensure_ascii=False),
                    "primary_function": mashup_tags.get("primary_function", ""),
                    "style_effect": mashup_tags.get("style_effect", ""),
                    # å‰ªè¾‘å‚æ•° (ç²¾ç®€)
                    "rhythm": editing_params.get("rhythm", ""),
                    "duration": editing_params.get("duration", 0)
                })
            
            # è·å–å‘é‡
            try:
                embeddings = self.embedding.embed(texts)
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} å‘é‡åŒ–å¤±è´¥: {e}")
                continue
            
            # å­˜å…¥æ•°æ®åº“
            try:
                self.store.upsert_lines(
                    ids=ids,
                    embeddings=embeddings,
                    documents=documents,
                    metadatas=metadatas
                )
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} å­˜å‚¨å¤±è´¥: {e}")
                continue
            
            processed += len(batch)
            
            # è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(processed, total)
            
            # æ§åˆ¶å°è¿›åº¦
            if (i // batch_size + 1) % 5 == 0 or i + batch_size >= total:
                elapsed = time.time() - start_time
                speed = processed / elapsed if elapsed > 0 else 0
                print(f"ğŸ”„ è¿›åº¦: {processed}/{total} ({processed/total:.1%}) | é€Ÿåº¦: {speed:.1f}æ¡/ç§’")
        
        print(f"âœ… å‘é‡åŒ–å®Œæˆï¼å…±å¤„ç† {processed} æ¡ï¼Œè€—æ—¶ {time.time() - start_time:.1f}ç§’")
        
        return processed
    
    def search(
        self,
        query: str,
        n_results: int = 10,
        filters: Dict = None
    ) -> List[Dict]:
        """æœç´¢å°è¯"""
        return self.store.search_by_text(
            query_text=query,
            embedding_provider=self.embedding,
            n_results=n_results,
            where=filters
        )
    
    def find_next_lines(
        self,
        current_line_id: str,
        n_results: int = 10
    ) -> List[Dict]:
        """æŸ¥æ‰¾èƒ½æ¥çš„ä¸‹ä¸€å¥"""
        return self.store.search_next_line(
            current_line_id=current_line_id,
            embedding_provider=self.embedding,
            n_results=n_results
        )
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.store.get_stats()


# ==================== CLI ====================
def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="å°è¯å‘é‡åŒ–å…¥åº“å·¥å…·")
    parser.add_argument("action", choices=["vectorize", "search", "stats", "reset"],
                       help="æ“ä½œç±»å‹: vectorize=å‘é‡åŒ–, search=æœç´¢, stats=ç»Ÿè®¡, reset=é‡ç½®")
    parser.add_argument("--input", help="æ ‡æ³¨JSONæ–‡ä»¶è·¯å¾„ (vectorizeæ¨¡å¼)")
    parser.add_argument("--query", help="æœç´¢æŸ¥è¯¢ (searchæ¨¡å¼)")
    parser.add_argument("--limit", type=int, default=10, help="ç»“æœæ•°é‡")
    parser.add_argument("--batch-size", type=int, default=50, help="æ‰¹å¤„ç†å¤§å°")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¬ å°è¯å‘é‡åŒ–å·¥å…·")
    print("=" * 60)
    
    vectorizer = Vectorizer()
    
    if args.action == "vectorize":
        if not args.input:
            print("âŒ è¯·æŒ‡å®šè¾“å…¥æ–‡ä»¶: --input <path>")
            return
        vectorizer.vectorize_annotations(args.input, batch_size=args.batch_size)
    
    elif args.action == "search":
        if not args.query:
            print("âŒ è¯·æŒ‡å®šæœç´¢æŸ¥è¯¢: --query <text>")
            return
        
        results = vectorizer.search(args.query, n_results=args.limit)
        print(f"\nğŸ” æœç´¢ç»“æœ ({len(results)} æ¡):")
        for i, r in enumerate(results):
            meta = r['metadata']
            # å…¼å®¹æ–°æ—§æ ¼å¼
            movie = meta.get('media_id') or meta.get('movie_title') or meta.get('movie_id') or meta.get('source_movie', 'æœªçŸ¥')
            print(f"\n{i+1}. [{movie}]")
            print(f"   å°è¯: {r['text']}")
            print(f"   å¥å‹: {meta.get('sentence_type', '')} | æƒ…ç»ª: {meta.get('emotion', '')}")
            print(f"   ç›¸ä¼¼åº¦: {r['score']:.2%}")
    
    elif args.action == "stats":
        stats = vectorizer.get_stats()
        print(f"\nğŸ“Š å‘é‡åº“ç»Ÿè®¡:")
        print(f"   æ€»å°è¯æ•°: {stats['total_lines']}")
        print(f"\n   ç”µå½±åˆ†å¸ƒ:")
        for movie, count in stats.get("movies", {}).items():
            print(f"     {movie}: {count} æ¡")
        print(f"\n   å¥å‹åˆ†å¸ƒ:")
        for st, count in list(stats.get("sentence_types", {}).items())[:10]:
            print(f"     {st}: {count} æ¡")
    
    elif args.action == "reset":
        confirm = input("âš ï¸ ç¡®å®šè¦é‡ç½®å‘é‡åº“å—ï¼Ÿè¿™å°†åˆ é™¤æ‰€æœ‰æ•°æ®ï¼(y/N): ")
        if confirm.lower() == 'y':
            vectorizer.store.reset()
        else:
            print("å·²å–æ¶ˆ")


if __name__ == "__main__":
    main()
