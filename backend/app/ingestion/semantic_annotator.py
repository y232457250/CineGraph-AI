# backend/app/ingestion/semantic_annotator.py
"""
è¯­ä¹‰æ ‡æ³¨å™¨ - ç”¨äºå°è¯æ··å‰ªçš„è¯­ä¹‰æ ‡æ³¨
æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œå•†ç”¨APIåˆ‡æ¢
"""

import os
import re
import json
import time
import yaml
import threading
import requests
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from enum import Enum

# ==================== é…ç½®è·¯å¾„ ====================
BASE_DIR = Path(__file__).parent.parent.parent.parent
CONFIG_DIR = BASE_DIR / "config"
MASHUP_CONFIG_PATH = CONFIG_DIR / "mashup_v5_config.json"
LLM_CONFIG_PATH = CONFIG_DIR / "llm_providers.yaml"
PROMPT_CONFIG_PATH = CONFIG_DIR / "prompt_config.json"


# ==================== è‹±æ–‡â†’ä¸­æ–‡æ˜ å°„è¡¨ ====================
SENTENCE_TYPE_MAP = {
    "question": "é—®å¥", "answer": "ç­”å¥", "command": "å‘½ä»¤", "threat": "å¨èƒ",
    "counter_question": "åé—®", "mock": "å˜²è®½", "refuse": "æ‹’ç»", "fear": "å®³æ€•",
    "surrender": "æ±‚é¥¶", "counter_attack": "åå‡»", "anger": "æ„¤æ€’", "exclaim": "æ„Ÿå¹",
    "persuade": "åŠè¯´", "agree": "åŒæ„", "action": "è¡ŒåŠ¨", "interrupt": "æ‰“æ–­",
    "reveal": "æ­ç¤º", "obey": "æœä»", "comment": "è¯„è®º", "shock": "éœ‡æƒŠ",
    "interjection": "æ„Ÿå¹", "statement": "é™ˆè¿°"
}

EMOTION_MAP = {
    "angry": "æ„¤æ€’", "rage": "ç‹‚æ€’", "fear": "å®³æ€•", "mock": "å˜²è®½",
    "proud": "å¾—æ„", "arrogant": "åš£å¼ ", "helpless": "æ— å¥ˆ", "calm": "å†·é™",
    "shock": "éœ‡æƒŠ", "funny": "æç¬‘", "absurd": "è’è¯", "tsundere": "å‚²å¨‡"
}

TONE_MAP = {
    "strong": "å¼ºç¡¬", "weak": "è½¯å¼±", "provocative": "æŒ‘è¡…", "humble": "å‘å¾®",
    "arrogant": "å‚²æ…¢", "questioning": "è´¨ç–‘", "certain": "è‚¯å®š", "hesitant": "çŠ¹è±«",
    "pleading": "æ³æ±‚", "threatening": "å¨èƒ"
}

CHARACTER_TYPE_MAP = {
    "emperor": "çš‡å¸", "official": "å¤§è‡£", "hero": "è‹±é›„", "villain": "åæ´¾",
    "comic": "æç¬‘è§’è‰²", "victim": "å—å®³è€…", "bystander": "æ—è§‚è€…", "wise": "æ™ºè€…"
}

def to_chinese(value: str, mapping: Dict[str, str]) -> str:
    """å°†è‹±æ–‡æ ‡ç­¾è½¬æ¢ä¸ºä¸­æ–‡ï¼Œå¦‚æœå·²ç»æ˜¯ä¸­æ–‡åˆ™ä¿æŒä¸å˜"""
    if not value:
        return value
    # å»é™¤å¯èƒ½çš„æ‹¬å·æ³¨é‡Šï¼Œå¦‚ "ç­”å¥(answer)" -> "ç­”å¥"
    clean_value = value.split("(")[0].strip()
    # å¦‚æœæ˜¯è‹±æ–‡keyï¼Œè½¬ä¸ºä¸­æ–‡
    if clean_value.lower() in mapping:
        return mapping[clean_value.lower()]
    # å¦‚æœå€¼æœ¬èº«å°±æ˜¯ä¸­æ–‡åç§°ï¼Œç›´æ¥è¿”å›
    if clean_value in mapping.values():
        return clean_value
    return clean_value


# ==================== æ•°æ®ç±» ====================
@dataclass
class SourceInfo:
    """ğŸ“ æ¥æºå®šä½ä¿¡æ¯ - ç²¾ç®€ç‰ˆ
    å®Œæ•´åª’ä½“ä¿¡æ¯é€šè¿‡media_idå…³è”media_index.jsonè·å–
    """
    media_id: str = ""      # å…³è”media_indexçš„key
    start: float = 0.0      # å¼€å§‹æ—¶é—´(ç§’)
    end: float = 0.0        # ç»“æŸæ—¶é—´(ç§’)
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class MashupTags:
    """ğŸ­ æ··å‰ªæ ¸å¿ƒæ ‡ç­¾ (ç”¨äºæœç´¢å’ŒåŒ¹é…)"""
    # å¥å‹åˆ†ç±» - å†³å®šèƒ½æ¥ä»€ä¹ˆ
    sentence_type: str = ""  # é—®å¥|ç­”å¥|æ„Ÿå¹|å‘½ä»¤|è´¨é—®|å¨èƒ|æ‹’ç»|æ±‚é¥¶|å˜²è®½
    
    # æƒ…ç»ªæ ‡ç­¾ - ç”¨äºæƒ…ç»ªåŒ¹é…
    emotion: str = ""  # æ„¤æ€’|æç¬‘|å®³æ€•|å˜²è®½|æ³æ±‚|å€”å¼º|å¾—æ„|æ— å¥ˆ|ç‹‚èº
    
    # è¯­æ°”æ ‡ç­¾ - ç”¨äºèŠ‚å¥æ§åˆ¶
    tone: str = ""  # å¼ºç¡¬|è½¯å¼±|æŒ‘è¡…|æ— å¥ˆ|å‚²æ…¢|å‘å¾®|ç–‘æƒ‘|è‚¯å®š
    
    # æ ¸å¿ƒåŠŸèƒ½
    primary_function: str = ""  # å¼ºè¡Œè§£é‡Š|èº«ä»½åè½¬|åœºæ™¯å«æ¥|é‡‘å¥å¼•ç”¨...
    
    # é£æ ¼æ•ˆæœ
    style_effect: str = ""  # åè®½é«˜çº§é»‘|è‡ªå˜²è§£æ„|è°éŸ³æ¢—ç‹...
    
    # â­ æ¥è¯è§„åˆ™ - æ··å‰ªæ ¸å¿ƒ
    can_follow: List[str] = None  # èƒ½æ¥åœ¨ä»€ä¹ˆç±»å‹åé¢
    can_lead_to: List[str] = None  # åé¢èƒ½æ¥ä»€ä¹ˆ
    
    # å…³é”®è¯ (ç”¨äºç²¾å‡†æœç´¢)
    keywords: List[str] = None
    
    # è§’è‰²ç±»å‹ (è·¨å‰§æ¥è¯ç”¨)
    character_type: str = ""  # çš‡å¸|å¤§è‡£|å¦–æ€ª|è‹±é›„|å—å®³è€…|æ–½æš´è€…|æ—è§‚è€…
    
    def __post_init__(self):
        if self.can_follow is None:
            self.can_follow = []
        if self.can_lead_to is None:
            self.can_lead_to = []
        if self.keywords is None:
            self.keywords = []
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class EditingParams:
    """ğŸ“Š å‰ªè¾‘å‚æ•° - ç²¾ç®€ç‰ˆ"""
    rhythm: str = ""        # å¿«é€Ÿåˆ‡æ¢—|æ…¢æ”¾æ‰“è„¸|æˆ›ç„¶è€Œæ­¢...
    duration: float = 0.0   # æ—¶é•¿(ç§’)
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class LineAnnotation:
    """å•å¥å°è¯æ ‡æ³¨ç»“æœ - ç²¾ç®€ç‰ˆæ··å‰ªè§„èŒƒ
    
    è®¾è®¡åŸåˆ™ï¼š
    1. sourceåªä¿ç•™å®šä½å¿…éœ€ä¿¡æ¯ï¼Œå®Œæ•´åª’ä½“ä¿¡æ¯é€šè¿‡media_idå…³è”
    2. åˆ é™¤è°ƒè¯•ç”¨å…ƒæ•°æ®(llm_provider, config_version)
    3. åˆ é™¤å®ç”¨æ€§ä½çš„å­—æ®µ(audio_suggest)
    4. æ‰€æœ‰æ ‡ç­¾ç»Ÿä¸€ä½¿ç”¨ä¸­æ–‡
    """
    id: str
    text: str
    
    # ğŸ“ æ¥æºå®šä½ (ç²¾ç®€)
    source: SourceInfo = None
    
    # ğŸ­ æ··å‰ªæ ¸å¿ƒæ ‡ç­¾
    mashup_tags: MashupTags = None
    
    # ğŸ” å‘é‡åŒ–æ–‡æœ¬ (ç”¨äºembedding)
    vector_text: str = ""
    
    # ğŸ“Š å‰ªè¾‘å‚æ•° (ç²¾ç®€)
    editing_params: EditingParams = None
    
    # è¯­ä¹‰æ‘˜è¦
    semantic_summary: str = ""
    
    # æ ‡æ³¨æ—¶é—´æˆ³
    annotated_at: float = 0
    
    def __post_init__(self):
        if self.source is None:
            self.source = SourceInfo()
        if self.mashup_tags is None:
            self.mashup_tags = MashupTags()
        if self.editing_params is None:
            self.editing_params = EditingParams()
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸ï¼Œä¿æŒåµŒå¥—ç»“æ„"""
        return {
            "id": self.id,
            "text": self.text,
            "source": self.source.to_dict() if self.source else {},
            "mashup_tags": self.mashup_tags.to_dict() if self.mashup_tags else {},
            "vector_text": self.vector_text,
            "editing_params": self.editing_params.to_dict() if self.editing_params else {},
            "semantic_summary": self.semantic_summary,
            "annotated_at": self.annotated_at
        }
    
    def generate_vector_text(self):
        """ç”Ÿæˆç”¨äºå‘é‡åŒ–çš„æ–‡æœ¬ - çº¯ä¸­æ–‡ç®€æ´æ ¼å¼"""
        tags = self.mashup_tags
        parts = [
            tags.sentence_type,
            tags.emotion,
            tags.tone,
            self.text,
        ]
        # æ·»åŠ å¯å¼•å¯¼çš„ç±»å‹
        if tags.can_lead_to:
            parts.extend(tags.can_lead_to)
        # æ·»åŠ å…³é”®è¯
        if tags.keywords:
            parts.extend(tags.keywords)
        
        self.vector_text = " ".join(filter(None, parts))
    
    @classmethod
    def from_dict(cls, d: Dict) -> "LineAnnotation":
        """ä»å­—å…¸æ¢å¤ LineAnnotation å¯¹è±¡ï¼ˆç”¨äº checkpoint æ¢å¤ï¼‰"""
        source_d = d.get("source", {})
        source = SourceInfo(
            media_id=source_d.get("media_id", ""),
            start=source_d.get("start", 0.0),
            end=source_d.get("end", 0.0)
        )
        tags_d = d.get("mashup_tags", {})
        mashup_tags = MashupTags(
            sentence_type=tags_d.get("sentence_type", ""),
            emotion=tags_d.get("emotion", ""),
            tone=tags_d.get("tone", ""),
            primary_function=tags_d.get("primary_function", ""),
            style_effect=tags_d.get("style_effect", ""),
            can_follow=tags_d.get("can_follow", []),
            can_lead_to=tags_d.get("can_lead_to", []),
            keywords=tags_d.get("keywords", []),
            character_type=tags_d.get("character_type", "")
        )
        ep_d = d.get("editing_params", {})
        editing_params = EditingParams(
            rhythm=ep_d.get("rhythm", ""),
            duration=ep_d.get("duration", 0.0)
        )
        return cls(
            id=d.get("id", ""),
            text=d.get("text", ""),
            source=source,
            mashup_tags=mashup_tags,
            vector_text=d.get("vector_text", ""),
            editing_params=editing_params,
            semantic_summary=d.get("semantic_summary", ""),
            annotated_at=d.get("annotated_at", 0)
        )


# ==================== LLMæä¾›è€…ç®¡ç† ====================
# ä½¿ç”¨ç‹¬ç«‹çš„LLMæ¨¡å—ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ç±»å‹
from app.llm import LLMProviderManager


# ==================== é…ç½®åŠ è½½ ====================
class MashupConfig:
    """æ··å‰ªé…ç½®ç®¡ç†"""
    
    def __init__(self, config_path: Path = MASHUP_CONFIG_PATH):
        self.config_path = config_path
        self.config: Dict = {}
        self._load_config()
    
    def _load_config(self):
        """åŠ è½½é…ç½®"""
        if not self.config_path.exists():
            print(f"âš ï¸ æ··å‰ªé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            self.config = {"version": "default"}
            return
        
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                self.config = json.load(f)
            print(f"âœ… æ··å‰ªé…ç½®åŠ è½½æˆåŠŸ: {self.config.get('version', 'unknown')}")
        except Exception as e:
            print(f"âš ï¸ æ··å‰ªé…ç½®åŠ è½½å¤±è´¥: {e}")
            self.config = {"version": "default"}
    
    @property
    def version(self) -> str:
        return self.config.get("version", "unknown")
    
    @property
    def sentence_types(self) -> List[Dict]:
        return self.config.get("sentence_types", {}).get("types", [])
    
    @property
    def emotions(self) -> List[Dict]:
        return self.config.get("emotions", {}).get("types", [])
    
    @property
    def tones(self) -> List[Dict]:
        return self.config.get("tones", {}).get("types", [])
    
    @property
    def character_types(self) -> List[Dict]:
        return self.config.get("character_types", {}).get("types", [])
    
    @property
    def primary_functions(self) -> List[str]:
        return self.config.get("primary_functions", [])
    
    @property
    def style_effects(self) -> List[str]:
        return self.config.get("style_effects", [])
    
    def get_sentence_type_names(self) -> List[str]:
        return [t["name"] for t in self.sentence_types]
    
    def get_emotion_names(self) -> List[str]:
        return [e["name"] for e in self.emotions]
    
    def get_tone_names(self) -> List[str]:
        return [t["name"] for t in self.tones]
    
    def get_can_follow_for_type(self, type_id: str) -> List[str]:
        """è·å–æŸä¸ªå¥å‹èƒ½æ¥ä»€ä¹ˆç±»å‹"""
        for t in self.sentence_types:
            if t["id"] == type_id:
                return t.get("can_follow", [])
        return []


# ==================== æç¤ºè¯æ„å»º ====================
def load_prompt_config() -> Dict:
    """åŠ è½½æç¤ºè¯é…ç½®"""
    if PROMPT_CONFIG_PATH.exists():
        try:
            with open(PROMPT_CONFIG_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æç¤ºè¯é…ç½®å¤±è´¥: {e}")
    return {}


def build_annotation_prompt(
    current_line: str, 
    context_lines: List[str], 
    config: MashupConfig
) -> Tuple[str, str]:
    """æ„å»ºè¯­ä¹‰æ ‡æ³¨æç¤ºè¯"""
    
    # å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰æç¤ºè¯
    prompt_config = load_prompt_config()
    annotation_cfg = prompt_config.get("annotation_prompt", {})
    custom_system = annotation_cfg.get("system_prompt", "")
    custom_template = annotation_cfg.get("user_prompt_template", "")
    custom_output_format = annotation_cfg.get("output_format", None)
    
    sentence_types = ", ".join([f"{t['name']}({t['id']})" for t in config.sentence_types[:10]])
    emotions = ", ".join([e['name'] for e in config.emotions])
    tones = ", ".join([t['name'] for t in config.tones])
    char_types = ", ".join([c['name'] for c in config.character_types])
    primary_funcs = ", ".join(config.primary_functions[:8])
    style_effects = ", ".join(config.style_effects[:8])
    
    # ä½¿ç”¨è‡ªå®šä¹‰æˆ–é»˜è®¤ç³»ç»Ÿæç¤ºè¯
    system_prompt = custom_system if custom_system else """ä½ æ˜¯ä¸“ä¸šçš„å½±è§†æ··å‰ªåˆ›ä½œä¸“å®¶ï¼Œæ“…é•¿åˆ†æå°è¯åœ¨æ··å‰ªä¸­çš„ä½¿ç”¨æ½œåŠ›ã€‚
ä½ éœ€è¦åˆ†ææ¯å¥å°è¯çš„å¥å‹ã€æƒ…ç»ªã€è¯­æ°”ç­‰ç‰¹å¾ï¼Œå¸®åŠ©åˆ›ä½œè€…æ‰¾åˆ°èƒ½"æ¥ä¸Š"çš„ä¸‹ä¸€å¥å°è¯ã€‚
è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜ã€‚"""
    
    default_user_prompt = f"""
## ä»»åŠ¡
åˆ†æä»¥ä¸‹å°è¯åœ¨**è„±ç¦»åŸç‰‡è¯­å¢ƒ**åçš„æ··å‰ªæ½œåŠ›ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. è¿™å¥è¯æ˜¯ä»€ä¹ˆç±»å‹ï¼Ÿï¼ˆé—®å¥ï¼Ÿå‘½ä»¤ï¼Ÿå¨èƒï¼Ÿå˜²è®½ï¼Ÿï¼‰
2. è¿™å¥è¯åé¢èƒ½æ¥ä»€ä¹ˆç±»å‹çš„å°è¯ï¼Ÿ
3. è¿™å¥è¯é€‚åˆæ¥åœ¨ä»€ä¹ˆç±»å‹çš„å°è¯åé¢ï¼Ÿ

## å½“å‰å°è¯
"{current_line}"

## ä¸Šä¸‹æ–‡å‚è€ƒ
{json.dumps(context_lines, ensure_ascii=False)}

## å¯é€‰æ ‡ç­¾

### å¥å‹åˆ†ç±»ï¼ˆå¿…é€‰ä¸€ä¸ªï¼‰
{sentence_types}

### æƒ…ç»ªæ ‡ç­¾ï¼ˆå¿…é€‰ä¸€ä¸ªï¼‰
{emotions}

### è¯­æ°”æ ‡ç­¾ï¼ˆå¿…é€‰ä¸€ä¸ªï¼‰
{tones}

### è§’è‰²ç±»å‹ï¼ˆå¿…é€‰ä¸€ä¸ªï¼‰
{char_types}

### æ··å‰ªåŠŸèƒ½ï¼ˆé€‰æœ€åˆé€‚çš„ï¼‰
{primary_funcs}

### é£æ ¼æ•ˆæœï¼ˆé€‰æœ€åˆé€‚çš„ï¼‰
{style_effects}

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰
{{
  "sentence_type": "å¥å‹IDï¼ˆå¦‚question, threat, mockç­‰ï¼‰",
  "emotion": "æƒ…ç»ªåç§°",
  "tone": "è¯­æ°”åç§°",
  "character_type": "è§’è‰²ç±»å‹åç§°",
  "can_follow": ["èƒ½æ¥åœ¨ä»€ä¹ˆå¥å‹åé¢", "æœ€å¤š3ä¸ª"],
  "can_lead_to": ["åé¢èƒ½æ¥ä»€ä¹ˆå¥å‹", "æœ€å¤š3ä¸ª"],
  "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "æœ€å¤š3ä¸ª"],
  "primary_function": "æ··å‰ªåŠŸèƒ½",
  "style_effect": "é£æ ¼æ•ˆæœ",
  "editing_rhythm": "å‰ªè¾‘èŠ‚å¥å»ºè®®",
  "audio_suggest": ["éŸ³æ•ˆå»ºè®®1", "éŸ³æ•ˆå»ºè®®2"],
  "semantic_summary": "ä¸€å¥è¯æè¿°è¿™å¥å°è¯çš„æ··å‰ªç”¨é€”ï¼ˆ20å­—ä»¥å†…ï¼‰"
}}
"""

    # å¦‚æœé…ç½®ä¸­æä¾›äº†è‡ªå®šä¹‰æ¨¡æ¿å’Œè¾“å‡ºæ ¼å¼ï¼Œä¼˜å…ˆä½¿ç”¨
    if custom_template:
        try:
            output_format_str = ""
            if isinstance(custom_output_format, dict):
                output_format_str = json.dumps(custom_output_format, ensure_ascii=False, indent=2)
            elif custom_output_format:
                output_format_str = str(custom_output_format)
            else:
                output_format_str = "{}"

            user_prompt = custom_template.format(
                current_line=current_line,
                context_lines=json.dumps(context_lines, ensure_ascii=False),
                sentence_types=sentence_types,
                emotions=emotions,
                tones=tones,
                character_types=char_types,
                primary_functions=primary_funcs,
                style_effects=style_effects,
                output_format=output_format_str
            )
        except Exception as e:
            print(f"âš ï¸ è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿æ¸²æŸ“å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {e}")
            user_prompt = default_user_prompt
    else:
        user_prompt = default_user_prompt
    
    return system_prompt, user_prompt


def build_batch_annotation_prompt(
    lines_batch: List[Dict],  # [{"idx": 0, "text": "...", "context": [...]}]
    config: MashupConfig
) -> Tuple[str, str]:
    """æ„å»ºæ‰¹é‡è¯­ä¹‰æ ‡æ³¨æç¤ºè¯ - ä¸€æ¬¡å¤„ç†å¤šè¡Œå°è¯"""
    
    sentence_types = ", ".join([f"{t['name']}({t['id']})" for t in config.sentence_types[:10]])
    emotions = ", ".join([e['name'] for e in config.emotions])
    tones = ", ".join([t['name'] for t in config.tones])
    char_types = ", ".join([c['name'] for c in config.character_types])
    primary_funcs = ", ".join(config.primary_functions[:8])
    style_effects = ", ".join(config.style_effects[:8])
    
    system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å½±è§†æ··å‰ªåˆ›ä½œä¸“å®¶ï¼Œæ“…é•¿åˆ†æå°è¯åœ¨æ··å‰ªä¸­çš„ä½¿ç”¨æ½œåŠ›ã€‚
ä½ éœ€è¦æ‰¹é‡åˆ†æå¤šå¥å°è¯çš„å¥å‹ã€æƒ…ç»ªã€è¯­æ°”ç­‰ç‰¹å¾ã€‚

é‡è¦è¦æ±‚ï¼š
1. å¿…é¡»ä¸ºæ¯ä¸€å¥å°è¯éƒ½ç”Ÿæˆæ ‡æ³¨ï¼Œä¸èƒ½é—æ¼ä»»ä½•ä¸€å¥
2. è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å« results æ•°ç»„ï¼Œé•¿åº¦å¿…é¡»ä¸º {len(lines_batch)}
3. results æ•°ç»„é¡ºåºå¿…é¡»ä¸è¾“å…¥å°è¯é¡ºåºå®Œå…¨ä¸€è‡´
4. åªè¾“å‡ºJSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–è¯´æ˜æ–‡å­—

æ¯ä¸ªæ ‡æ³¨å¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- line_index: å°è¯åºå·ï¼ˆä»1å¼€å§‹ï¼‰
- sentence_type: å¥å‹åˆ†ç±»
- emotion: æƒ…ç»ªæ ‡ç­¾
- tone: è¯­æ°”æ ‡ç­¾
- character_type: è§’è‰²ç±»å‹
- can_follow: èƒ½æ¥åœ¨ä»€ä¹ˆå¥å‹åé¢çš„æ•°ç»„
- can_lead_to: åé¢èƒ½æ¥ä»€ä¹ˆå¥å‹çš„æ•°ç»„
- keywords: å…³é”®è¯æ•°ç»„
- primary_function: æ··å‰ªåŠŸèƒ½
- style_effect: é£æ ¼æ•ˆæœ
- semantic_summary: æ··å‰ªç”¨é€”æè¿°"""
    
    # æ„å»ºæ‰¹é‡å°è¯åˆ—è¡¨
    lines_text = "\n".join([f'{i+1}. "{item["text"]}"' for i, item in enumerate(lines_batch)])
    
    user_prompt = f"""
## ä»»åŠ¡
æ‰¹é‡åˆ†æä»¥ä¸‹ {len(lines_batch)} å¥å°è¯åœ¨**è„±ç¦»åŸç‰‡è¯­å¢ƒ**åçš„æ··å‰ªæ½œåŠ›ã€‚

## å¾…åˆ†æå°è¯
{lines_text}

## å¯é€‰æ ‡ç­¾

### å¥å‹åˆ†ç±»
{sentence_types}

### æƒ…ç»ªæ ‡ç­¾
{emotions}

### è¯­æ°”æ ‡ç­¾
{tones}

### è§’è‰²ç±»å‹
{char_types}

### æ··å‰ªåŠŸèƒ½
{primary_funcs}

### é£æ ¼æ•ˆæœ
{style_effects}

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONå¯¹è±¡ï¼Œresultsä¸ºæ•°ç»„ï¼‰
{{
    "results": [
        {{
            "line_index": 1,
            "sentence_type": "å¥å‹ID",
            "emotion": "æƒ…ç»ªåç§°",
            "tone": "è¯­æ°”åç§°",
            "character_type": "è§’è‰²ç±»å‹",
            "can_follow": ["èƒ½æ¥åœ¨ä»€ä¹ˆå¥å‹åé¢"],
            "can_lead_to": ["åé¢èƒ½æ¥ä»€ä¹ˆå¥å‹"],
            "keywords": ["å…³é”®è¯"],
            "primary_function": "æ··å‰ªåŠŸèƒ½",
            "style_effect": "é£æ ¼æ•ˆæœ",
            "semantic_summary": "æ··å‰ªç”¨é€”æè¿°"
        }}
    ]
}}
"""
    
    return system_prompt, user_prompt


def parse_batch_llm_response(response_text: str) -> List[Dict]:
    """è§£ææ‰¹é‡æ ‡æ³¨çš„LLMå“åº”ï¼Œè¿”å›ç»“æœåˆ—è¡¨
    
    å¢å¼ºç‰ˆï¼šå¤„ç†å„ç§æ¨¡å‹è¿”å›çš„éæ ‡å‡†æ ¼å¼ï¼ŒåŒ…æ‹¬ï¼š
    - Markdownä»£ç å—
    - <think>æ ‡ç­¾ï¼ˆQwen3ç­‰æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼‰
    - å…¶ä»–éJSONå†…å®¹
    """
    if not response_text:
        print("âš ï¸ æ‰¹é‡å“åº”è§£æå¤±è´¥: å“åº”ä¸ºç©º")
        return []
    
    try:
        # æ¸…ç†å“åº”æ–‡æœ¬
        cleaned = response_text.strip()
        
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        cleaned = re.sub(r'```(?:json)?', '', cleaned)
        cleaned = re.sub(r'```', '', cleaned)
        
        # ç§»é™¤<think>...</think>æ ‡ç­¾ï¼ˆQwen3ç­‰æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼‰
        cleaned = re.sub(r'<think>.*?</think>', '', cleaned, flags=re.DOTALL)
        
        # ç§»é™¤å…¶ä»–å¯èƒ½çš„æ ‡ç­¾
        cleaned = re.sub(r'<[^>]+>.*?</[^>]+>', '', cleaned, flags=re.DOTALL)
        
        # å»é™¤é¦–å°¾ç©ºç™½
        cleaned = cleaned.strip()

        # 1) å…ˆå°è¯•ç›´æ¥è§£æå®Œæ•´JSON
        try:
            parsed = json.loads(cleaned)
            if isinstance(parsed, list):
                return parsed
            if isinstance(parsed, dict):
                # å…¼å®¹å¸¸è§åŒ…è£…å­—æ®µ
                for key in ("results", "items", "data", "annotations", "outputs", "output", "choices"):
                    val = parsed.get(key)
                    if isinstance(val, list):
                        return val
                # å•å¯¹è±¡è¿”å›ï¼ŒåŒ…è£…æˆåˆ—è¡¨
                print("âš ï¸ æ‰¹é‡å“åº”è¿”å›äº†å•ä¸ªå¯¹è±¡ï¼ŒåŒ…è£…ä¸ºåˆ—è¡¨")
                return [parsed]
        except json.JSONDecodeError:
            pass

        # 2) æå–é¡¶å±‚JSONæ•°ç»„ï¼ˆé¿å…è¯¯æˆªå–å¯¹è±¡å†…æ•°ç»„å­—æ®µï¼‰
        def _extract_top_level_array(text: str):
            in_str = False
            escape = False
            depth = 0
            start = None
            for i, ch in enumerate(text):
                if escape:
                    escape = False
                    continue
                if ch == "\\":
                    escape = True
                    continue
                if ch == '"':
                    in_str = not in_str
                    continue
                if in_str:
                    continue
                if ch == '[':
                    if depth == 0:
                        start = i
                    depth += 1
                elif ch == ']' and depth > 0:
                    depth -= 1
                    if depth == 0 and start is not None:
                        return text[start:i + 1]
            return None

        array_json = _extract_top_level_array(cleaned)
        if array_json:
            try:
                results = json.loads(array_json)
                if isinstance(results, list):
                    return results
            except json.JSONDecodeError as je:
                print(f"âš ï¸ JSONæ•°ç»„è§£æå¤±è´¥: {je}")
                print(f"   æå–çš„JSONç‰‡æ®µ: {array_json[:200] if len(array_json) > 200 else array_json}")

        # 3) å°è¯•æå–é¡¶å±‚å¯¹è±¡
        def _extract_top_level_object(text: str):
            in_str = False
            escape = False
            depth = 0
            start = None
            for i, ch in enumerate(text):
                if escape:
                    escape = False
                    continue
                if ch == "\\":
                    escape = True
                    continue
                if ch == '"':
                    in_str = not in_str
                    continue
                if in_str:
                    continue
                if ch == '{':
                    if depth == 0:
                        start = i
                    depth += 1
                elif ch == '}' and depth > 0:
                    depth -= 1
                    if depth == 0 and start is not None:
                        return text[start:i + 1]
            return None

        obj_json = _extract_top_level_object(cleaned)
        if obj_json:
            try:
                result = json.loads(obj_json)
                if isinstance(result, dict):
                    print("âš ï¸ æ‰¹é‡å“åº”è¿”å›äº†å•ä¸ªå¯¹è±¡ï¼ŒåŒ…è£…ä¸ºåˆ—è¡¨")
                    return [result]
            except json.JSONDecodeError as je:
                print(f"âš ï¸ JSONå¯¹è±¡è§£æå¤±è´¥: {je}")
                print(f"   æå–çš„JSONç‰‡æ®µ: {obj_json[:200] if len(obj_json) > 200 else obj_json}")
            
    except Exception as e:
        print(f"âš ï¸ æ‰¹é‡å“åº”è§£æå¤±è´¥: {e}")
        print(f"   å“åº”å‰300å­—ç¬¦: {response_text[:300] if len(response_text) > 300 else response_text}")
    
    return []


# ==================== å“åº”è§£æ ====================
def parse_llm_response(response_text: str) -> Dict:
    """
    è§£æLLMå“åº”
    å¢å¼ºç‰ˆï¼šå¤„ç†å„ç§æ¨¡å‹è¿”å›çš„éæ ‡å‡†æ ¼å¼
    """
    if not response_text:
        print("âš ï¸ JSONè§£æå¤±è´¥: å“åº”ä¸ºç©º")
        return {"__parse_failed__": True, "raw_output": response_text}
    
    # æ¸…ç†å“åº”
    clean_text = response_text.strip()
    
    # ç§»é™¤markdownä»£ç å—æ ‡è®°
    clean_text = re.sub(r'```(?:json)?', '', clean_text)
    clean_text = re.sub(r'```', '', clean_text)
    
    # ç§»é™¤<think>...</think>æ ‡ç­¾ï¼ˆQwen3ç­‰æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼‰
    clean_text = re.sub(r'<think>.*?</think>', '', clean_text, flags=re.DOTALL)
    
    # ç§»é™¤å…¶ä»–å¯èƒ½çš„æ ‡ç­¾
    clean_text = re.sub(r'<[^>]+>.*?</[^>]+>', '', clean_text, flags=re.DOTALL)
    
    # 1) ç›´æ¥å°è¯•è§£æ
    try:
        parsed = json.loads(clean_text)
        if isinstance(parsed, dict):
            return parsed
    except json.JSONDecodeError:
        pass
    
    # 2) æå–é¡¶å±‚å¯¹è±¡
    def _extract_top_level_object(text: str):
        in_str = False
        escape = False
        depth = 0
        start = None
        for i, ch in enumerate(text):
            if escape:
                escape = False
                continue
            if ch == "\\":
                escape = True
                continue
            if ch == '"':
                in_str = not in_str
                continue
            if in_str:
                continue
            if ch == '{':
                if depth == 0:
                    start = i
                depth += 1
            elif ch == '}' and depth > 0:
                depth -= 1
                if depth == 0 and start is not None:
                    return text[start:i + 1]
        return None
    
    obj_json = _extract_top_level_object(clean_text)
    if obj_json:
        try:
            return json.loads(obj_json)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
            print(f"   æå–çš„JSON: {obj_json[:200] if len(obj_json) > 200 else obj_json}")
    
    print("âš ï¸ JSONè§£æå¤±è´¥: æœªæ‰¾åˆ°æœ‰æ•ˆJSONå¯¹è±¡")
    print(f"   å“åº”å‰200å­—ç¬¦: {response_text[:200] if len(response_text) > 200 else response_text}")
    return {"__parse_failed__": True, "raw_output": response_text}


def normalize_annotation(parsed: Dict) -> Dict:
    """å°†ä¸åŒæ ¼å¼çš„LLMè¾“å‡ºç»Ÿä¸€åˆ°LineAnnotationå­—æ®µ"""
    if not isinstance(parsed, dict):
        return get_default_annotation()

    if "mashup_analysis" in parsed:
        mashup = parsed.get("mashup_analysis") or {}
        quick = mashup.get("quick_tags", {})
        semantic = mashup.get("semantic_summary", {})
        creative = mashup.get("creative_params", {})

        return {
            "sentence_type": parsed.get("sentence_type", ""),
            "emotion": parsed.get("emotion", ""),
            "tone": parsed.get("tone", ""),
            "character_type": parsed.get("character_type", ""),
            "can_follow": parsed.get("can_follow", []) or [],
            "can_lead_to": parsed.get("can_lead_to", []) or [],
            "keywords": semantic.get("keywords", []) or parsed.get("keywords", []) or [],
            "primary_function": quick.get("primary", "") or parsed.get("primary_function", ""),
            "style_effect": quick.get("style", "") or parsed.get("style_effect", ""),
            "editing_rhythm": quick.get("rhythm", "") or parsed.get("editing_rhythm", ""),
            "audio_suggest": creative.get("audio_suggestions", []) or parsed.get("audio_suggest", []) or [],
            "semantic_summary": semantic.get("brief", "") or semantic.get("use_case", "") or parsed.get("semantic_summary", ""),
            "mashup_analysis": mashup,
            "raw_output": parsed
        }

    # é»˜è®¤æ ¼å¼ï¼šé¿å… raw_output è‡ªå¼•ç”¨å¯¼è‡´é€’å½’
    safe_raw_output = dict(parsed)
    safe_raw_output.pop("raw_output", None)
    normalized = dict(parsed)
    normalized["raw_output"] = safe_raw_output
    return normalized


def get_default_annotation() -> Dict:
    """è·å–é»˜è®¤æ ‡æ³¨"""
    return {
        "sentence_type": "exclaim",
        "emotion": "calm",
        "tone": "certain",
        "character_type": "bystander",
        "can_follow": [],
        "can_lead_to": [],
        "keywords": [],
        "primary_function": "å…¶ä»–",
        "style_effect": "å…¶ä»–",
        "editing_rhythm": "å¸¸è§„å‰ªè¾‘",
        "audio_suggest": [],
        "semantic_summary": "å¸¸è§„å°è¯"
    }


def get_unknown_annotation() -> Dict:
    """è·å–è§£æå¤±è´¥æ—¶çš„æœªçŸ¥æ ‡æ³¨ï¼ˆé¿å…é»˜è®¤å€¼ï¼‰"""
    return {
        "sentence_type": "æœªçŸ¥",
        "emotion": "æœªçŸ¥",
        "tone": "æœªçŸ¥",
        "character_type": "æœªçŸ¥",
        "can_follow": [],
        "can_lead_to": [],
        "keywords": [],
        "primary_function": "æœªçŸ¥",
        "style_effect": "æœªçŸ¥",
        "editing_rhythm": "",
        "audio_suggest": [],
        "semantic_summary": "æœªèƒ½è§£æåˆ°æœ‰æ•ˆæ ‡æ³¨"
    }


# ==================== å­—å¹•è§£æ ====================
def parse_srt(file_path: str) -> List[Dict]:
    """è§£æSRTå­—å¹•æ–‡ä»¶"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        content = f.read()
    
    blocks = re.split(r"\n\s*\n", content.strip())
    lines = []
    
    for block in blocks:
        parts = block.strip().split("\n")
        if len(parts) < 3:
            continue
        try:
            time_range = parts[1]
            text = " ".join(parts[2:]).replace("\n", " ").strip()
            if not text or "-->" not in time_range:
                continue
            start_str, end_str = time_range.split(" --> ")
            lines.append({
                "text": text,
                "start": _time_to_seconds(start_str),
                "end": _time_to_seconds(end_str)
            })
        except Exception:
            continue
    
    return lines


def _time_to_seconds(time_str: str) -> float:
    """æ—¶é—´å­—ç¬¦ä¸²è½¬ç§’æ•°"""
    h, m, s_ms = time_str.replace(",", ".").split(":")
    return float(h) * 3600 + float(m) * 60 + float(s_ms)


# ==================== Checkpoint å·¥å…·å‡½æ•° ====================
ANNOTATION_DIR = Path(__file__).parent.parent.parent / "data" / "annotations"

def _checkpoint_path(movie_id: str) -> Path:
    """è·å– checkpoint æ–‡ä»¶è·¯å¾„"""
    return ANNOTATION_DIR / f"{movie_id}_checkpoint.json"

def _annotation_output_path(movie_id: str) -> Path:
    """è·å–æ ‡æ³¨è¾“å‡ºæ–‡ä»¶è·¯å¾„"""
    return ANNOTATION_DIR / f"{movie_id}_annotated.json"

def load_checkpoint(movie_id: str) -> Optional[Dict]:
    """åŠ è½½ checkpointï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    
    Returns:
        checkpoint dict with keys:
            movie_id, llm_provider, total_lines, completed_indices,
            last_save_time, subtitle_path, movie_name
        or None if no checkpoint
    """
    cp_path = _checkpoint_path(movie_id)
    if cp_path.exists():
        try:
            with open(cp_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½checkpointå¤±è´¥: {e}")
    return None

def delete_checkpoint(movie_id: str):
    """åˆ é™¤ checkpoint æ–‡ä»¶"""
    cp_path = _checkpoint_path(movie_id)
    if cp_path.exists():
        cp_path.unlink()
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤checkpoint: {cp_path.name}")


# ==================== è¯­ä¹‰æ ‡æ³¨å™¨ ====================
class SemanticAnnotator:
    """è¯­ä¹‰æ ‡æ³¨å™¨"""
    
    def __init__(self, llm_provider: str = None, max_retries: int = None, save_interval: int = None):
        self.mashup_config = MashupConfig()
        self.prompt_config = load_prompt_config()
        self.batch_settings = self.prompt_config.get("batch_settings", {})
        self.llm_manager = LLMProviderManager()
        
        # åŠ¨æ€å‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®
        if max_retries is not None:
            self.batch_settings["max_retries"] = max_retries
        if save_interval is not None:
            self.batch_settings["save_interval"] = save_interval
        
        if llm_provider:
            self.llm_manager.set_active_provider(llm_provider)
        
        self.llm = self.llm_manager.get_provider()
        self.provider_name = self.llm_manager.active_provider
        
        # æš‚åœäº‹ä»¶ï¼ˆåŒºåˆ«äºå–æ¶ˆï¼‰
        self._pause_event = threading.Event()
    
    def annotate_line(
        self, 
        text: str, 
        context_lines: List[str],
        source_movie: str = "",
        source_file: str = "",
        start: float = 0,
        end: float = 0,
        line_id: str = ""
    ) -> LineAnnotation:
        """æ ‡æ³¨å•è¡Œå°è¯"""
        
        system_prompt, user_prompt = build_annotation_prompt(
            text, context_lines, self.mashup_config
        )
        
        retry_on_failure = self.batch_settings.get("retry_on_failure", True)
        max_retries = int(self.batch_settings.get("max_retries", 2))
        attempts = 0

        while True:
            try:
                response = self.llm.chat(system_prompt, user_prompt)
                parsed = parse_llm_response(response)
                if isinstance(parsed, dict) and parsed.get("__parse_failed__"):
                    raise ValueError("LLMè¿”å›æ— æ³•è§£æçš„JSON")
                break
            except Exception as e:
                attempts += 1
                if retry_on_failure and attempts <= max_retries:
                    print(f"âš ï¸ æ ‡æ³¨å¤±è´¥ï¼Œé‡è¯• {attempts}/{max_retries}: {e}")
                    continue
                print(f"âŒ æ ‡æ³¨å¤±è´¥: {e}")
                parsed = get_unknown_annotation()
                break

        normalized = normalize_annotation(parsed)
        if isinstance(normalized, dict) and normalized.get("__parse_failed__"):
            normalized = get_unknown_annotation()
        
        # è®¡ç®—æ—¶é•¿
        duration = end - start if end > start else 0
        
        # ï¿½ è‹±æ–‡â†’ä¸­æ–‡è½¬æ¢
        sentence_type = to_chinese(normalized.get("sentence_type", ""), SENTENCE_TYPE_MAP)
        emotion = to_chinese(normalized.get("emotion", ""), EMOTION_MAP)
        tone = to_chinese(normalized.get("tone", ""), TONE_MAP)
        character_type = to_chinese(normalized.get("character_type", ""), CHARACTER_TYPE_MAP)
        
        # can_follow/can_lead_to ä¹Ÿè½¬ä¸­æ–‡
        can_follow = [to_chinese(t, SENTENCE_TYPE_MAP) for t in normalized.get("can_follow", [])]
        can_lead_to = [to_chinese(t, SENTENCE_TYPE_MAP) for t in normalized.get("can_lead_to", [])]
        
        # ğŸ“ æ„å»ºæ¥æºä¿¡æ¯ (ç²¾ç®€ç‰ˆ - åªä¿ç•™å®šä½å¿…éœ€ä¿¡æ¯)
        source_info = SourceInfo(
            media_id=source_movie,  # å…³è”media_indexçš„key
            start=start,
            end=end
        )
        
        # ğŸ­ æ„å»ºæ··å‰ªæ ¸å¿ƒæ ‡ç­¾ (å…¨ä¸­æ–‡)
        mashup_tags = MashupTags(
            sentence_type=sentence_type,
            emotion=emotion,
            tone=tone,
            primary_function=normalized.get("primary_function", ""),
            style_effect=normalized.get("style_effect", ""),
            can_follow=can_follow,
            can_lead_to=can_lead_to,
            keywords=normalized.get("keywords", []),
            character_type=character_type
        )
        
        # ğŸ“Š æ„å»ºå‰ªè¾‘å‚æ•° (ç²¾ç®€ç‰ˆ)
        editing_params = EditingParams(
            rhythm=normalized.get("editing_rhythm", ""),
            duration=round(duration, 2)
        )
        
        # æ„å»ºæ ‡æ³¨ç»“æœ (ç²¾ç®€ç‰ˆ)
        annotation = LineAnnotation(
            id=line_id,
            text=text,
            source=source_info,
            mashup_tags=mashup_tags,
            editing_params=editing_params,
            semantic_summary=normalized.get("semantic_summary", ""),
            annotated_at=time.time()
        )
        
        # ç”Ÿæˆå‘é‡åŒ–æ–‡æœ¬
        annotation.generate_vector_text()
        
        return annotation
    
    def annotate_batch(
        self,
        lines_batch: List[Dict],  # [{"idx": int, "text": str, "start": float, "end": float}]
        movie_name: str = "",
        movie_id: str = "",  # è±†ç“£IDï¼Œç”¨äºmedia_id
        subtitle_path: str = ""
    ) -> List[LineAnnotation]:
        """æ‰¹é‡æ ‡æ³¨å¤šè¡Œå°è¯ - ä¸€æ¬¡LLMè°ƒç”¨å¤„ç†å¤šè¡Œ
        
        Args:
            lines_batch: å¾…æ ‡æ³¨çš„å°è¯åˆ—è¡¨
            movie_name: å½±ç‰‡åç§°ï¼ˆç”¨äºæç¤ºè¯å’Œidç”Ÿæˆï¼‰
            movie_id: å½±ç‰‡è±†ç“£IDï¼ˆç”¨äºmedia_idå…³è”å½±ç‰‡åº“ï¼‰
            subtitle_path: å­—å¹•æ–‡ä»¶è·¯å¾„
        """""
        
        if not lines_batch:
            return []
        
        # æ„å»ºæ‰¹é‡æç¤ºè¯
        batch_items = [{"idx": item["idx"], "text": item["text"]} for item in lines_batch]
        system_prompt, user_prompt = build_batch_annotation_prompt(batch_items, self.mashup_config)
        
        retry_on_failure = self.batch_settings.get("retry_on_failure", True)
        max_retries = int(self.batch_settings.get("max_retries", 2))
        attempts = 0
        
        parsed_results = []
        raw_response = ""
        actual_media_id = movie_id if movie_id else movie_name
        while True:
            try:
                raw_response = self.llm.chat(system_prompt, user_prompt)
                parsed_results = parse_batch_llm_response(raw_response)
                
                # å¦‚æœè§£æç»“æœä¸ºç©ºæˆ–æ•°é‡ä¸åŒ¹é…ï¼Œè®°å½•è­¦å‘Š
                if len(parsed_results) == 0:
                    print(f"âš ï¸ æ‰¹é‡è§£æè¿”å›ç©ºç»“æœï¼Œå°è¯•é‡è¯•...")
                    attempts += 1
                    if retry_on_failure and attempts <= max_retries:
                        continue
                    print(f"âŒ æ‰¹é‡è§£æå¤šæ¬¡å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
                elif len(parsed_results) != len(lines_batch):
                    print(f"âš ï¸ æ‰¹é‡è§£æç»“æœæ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(lines_batch)}ï¼Œå®é™… {len(parsed_results)}")
                
                break
            except Exception as e:
                attempts += 1
                if retry_on_failure and attempts <= max_retries:
                    print(f"âš ï¸ æ‰¹é‡æ ‡æ³¨å¤±è´¥ï¼Œé‡è¯• {attempts}/{max_retries}: {e}")
                    continue
                print(f"âŒ æ‰¹é‡æ ‡æ³¨å¤±è´¥: {e}")
                break
        
        # å¦‚æœæ‰¹é‡ç»“æœä¸¥é‡ä¸è¶³ï¼Œæ”¹ç”¨å•è¡Œæ¨¡å¼è¡¥å…¨
        if len(parsed_results) < len(lines_batch):
            print("âš ï¸ æ‰¹é‡ç»“æœä¸è¶³ï¼Œå›é€€åˆ°å•è¡Œæ ‡æ³¨æ¨¡å¼è¡¥å…¨")
            fallback_results = []
            for item in lines_batch:
                idx = item["idx"]
                try:
                    ann = self.annotate_line(
                        text=item["text"],
                        context_lines=item.get("context", []),
                        source_movie=actual_media_id,
                        source_file=subtitle_path,
                        start=item["start"],
                        end=item["end"],
                        line_id=f"{actual_media_id}_line_{idx}"
                    )
                    fallback_results.append((idx, ann))
                except Exception as e:
                    print(f"âŒ å›é€€å•è¡Œæ ‡æ³¨å¤±è´¥: {e}")
                    fallback_results.append((idx, LineAnnotation(
                        id=f"{actual_media_id}_line_{idx}",
                        text=item["text"],
                        source=SourceInfo(
                            media_id=actual_media_id,
                            start=item["start"],
                            end=item["end"]
                        ),
                        annotated_at=time.time()
                    )))
            return fallback_results

        # å°†è§£æç»“æœæ˜ å°„å›åŸå§‹è¡Œ
        results = []
        
        # è®°å½•æ˜ å°„æƒ…å†µ
        parsed_count = len(parsed_results)
        batch_count = len(lines_batch)
        
        if parsed_count != batch_count:
            print(f"âš ï¸ æ‰¹é‡ç»“æœæ•°é‡ä¸åŒ¹é…: æœŸæœ› {batch_count}ï¼Œå®é™…è¿”å› {parsed_count}")
        
        # å¦‚æœè§£æç»“æœæ•°é‡ä¸æ‰¹æ¬¡å¤§å°ç›¸åŒï¼ŒæŒ‰é¡ºåºç›´æ¥æ˜ å°„
        use_sequential_mapping = parsed_count == batch_count
        
        # åˆ›å»ºä¸€ä¸ªé€šè¿‡line_indexæŸ¥æ‰¾çš„å­—å…¸ï¼Œç”¨äºç´¢å¼•åŒ¹é…
        index_map = {}
        for pr in parsed_results:
            line_idx = pr.get("line_index", pr.get("index", None))
            if line_idx is not None:
                index_map[line_idx] = pr
        
        for i, item in enumerate(lines_batch):
            idx = item["idx"]
            
            # ä»æ‰¹é‡ç»“æœä¸­æ‰¾åˆ°å¯¹åº”çš„æ ‡æ³¨
            parsed = None
            
            if use_sequential_mapping:
                # æŒ‰é¡ºåºåŒ¹é…ï¼ˆæ›´å¯é ï¼‰
                parsed = parsed_results[i]
            else:
                # å°è¯•é€šè¿‡ line_index åŒ¹é…ï¼ˆæ”¯æŒä»0æˆ–1å¼€å§‹ï¼‰
                if i + 1 in index_map:
                    parsed = index_map[i + 1]
                elif i in index_map:
                    parsed = index_map[i]
                elif i < parsed_count:
                    # å›é€€ï¼šå¦‚æœç´¢å¼•åœ¨èŒƒå›´å†…ï¼ŒæŒ‰é¡ºåºä½¿ç”¨
                    parsed = parsed_results[i]
            
            if parsed is None:
                # æŒ‰è¡Œè¡¥æ ‡æ³¨ï¼Œé¿å…é»˜è®¤å€¼
                print(f"âš ï¸ è¡Œ {i} æ— æ³•åŒ¹é…åˆ°æ ‡æ³¨ç»“æœï¼ŒæŒ‰è¡Œè¡¥æ ‡æ³¨")
                try:
                    parsed_line = self.annotate_line(
                        text=item["text"],
                        context_lines=item.get("context", []),
                        source_movie=actual_media_id,
                        source_file=subtitle_path,
                        start=item["start"],
                        end=item["end"],
                        line_id=f"{actual_media_id}_line_{idx}"
                    )
                    results.append((idx, parsed_line))
                    continue
                except Exception as e:
                    print(f"âŒ æŒ‰è¡Œè¡¥æ ‡æ³¨å¤±è´¥: {e}")
                    parsed = get_unknown_annotation()
            
            normalized = normalize_annotation(parsed)
            
            # è®¡ç®—æ—¶é•¿
            duration = item["end"] - item["start"] if item["end"] > item["start"] else 0
            
            # è‹±æ–‡â†’ä¸­æ–‡è½¬æ¢
            sentence_type = to_chinese(normalized.get("sentence_type", ""), SENTENCE_TYPE_MAP)
            emotion = to_chinese(normalized.get("emotion", ""), EMOTION_MAP)
            tone = to_chinese(normalized.get("tone", ""), TONE_MAP)
            character_type = to_chinese(normalized.get("character_type", ""), CHARACTER_TYPE_MAP)
            
            can_follow = [to_chinese(t, SENTENCE_TYPE_MAP) for t in normalized.get("can_follow", [])]
            can_lead_to = [to_chinese(t, SENTENCE_TYPE_MAP) for t in normalized.get("can_lead_to", [])]
            
            source_info = SourceInfo(
                media_id=actual_media_id,
                start=item["start"],
                end=item["end"]
            )
            
            mashup_tags = MashupTags(
                sentence_type=sentence_type,
                emotion=emotion,
                tone=tone,
                character_type=character_type,
                can_follow=can_follow,
                can_lead_to=can_lead_to,
                keywords=normalized.get("keywords", []),
                primary_function=normalized.get("primary_function", ""),
                style_effect=normalized.get("style_effect", "")
            )
            
            editing_params = EditingParams(
                rhythm=normalized.get("editing_rhythm", ""),
                duration=round(duration, 2)
            )
            
            annotation = LineAnnotation(
                id=f"{actual_media_id}_line_{idx}",
                text=item["text"],
                source=source_info,
                mashup_tags=mashup_tags,
                editing_params=editing_params,
                semantic_summary=normalized.get("semantic_summary", ""),
                annotated_at=time.time()
            )
            
            annotation.generate_vector_text()
            results.append((idx, annotation))
        
        return results
    
    def annotate_subtitle_file(
        self,
        subtitle_path: str,
        movie_name: str = "",
        movie_id: str = "",  # è±†ç“£IDï¼Œç”¨äºmedia_idå…³è”å½±ç‰‡åº“
        window_size: Optional[int] = None,
        max_workers: Optional[int] = None,
        batch_size: Optional[int] = None,
        progress_callback=None,
        cancel_event=None,
        pause_event=None,
        resume_from_checkpoint: bool = False
    ) -> List[LineAnnotation]:
        """æ ‡æ³¨æ•´ä¸ªå­—å¹•æ–‡ä»¶ï¼ˆæ”¯æŒå¢é‡ä¿å­˜å’Œæ–­ç‚¹ç»­æ ‡ï¼‰
        
        Args:
            subtitle_path: å­—å¹•æ–‡ä»¶è·¯å¾„
            movie_name: å½±ç‰‡åç§°ï¼ˆç”¨äºæç¤ºè¯å’Œæ˜¾ç¤ºï¼‰
            movie_id: å½±ç‰‡è±†ç“£IDï¼ˆç”¨äºmedia_idï¼Œä¾¿äºä¸å½±ç‰‡åº“å…³è”ï¼‰
            batch_size: æ¯æ¬¡LLMè°ƒç”¨å¤„ç†çš„å°è¯æ•°é‡ï¼ˆçœŸæ­£çš„æ‰¹å¤„ç†ï¼‰
            max_workers: å¹¶å‘çš„æ‰¹å¤„ç†ä»»åŠ¡æ•°
            window_size: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆç”¨äºå•è¡Œæ ‡æ³¨æ¨¡å¼ï¼‰
            progress_callback: è¿›åº¦å›è°ƒ
            cancel_event: å–æ¶ˆäº‹ä»¶
            pause_event: æš‚åœäº‹ä»¶ï¼ˆsetæ—¶æš‚åœï¼‰
            resume_from_checkpoint: æ˜¯å¦ä»checkpointæ¢å¤
        """
        
        # å¦‚æœæ²¡æœ‰æä¾›movie_idï¼Œä½¿ç”¨movie_nameä½œä¸ºå¤‡é€‰
        actual_media_id = movie_id if movie_id else movie_name

        if batch_size is None:
            batch_size = int(self.batch_settings.get("batch_size", 1))
        if max_workers is None:
            max_workers = int(self.batch_settings.get("max_concurrent_workers", 4))
        if window_size is None:
            window_size = int(self.batch_settings.get("context_window_size", 2))
        
        save_interval = int(self.batch_settings.get("save_interval", 50))
        
        # è§£æå­—å¹•
        lines = parse_srt(subtitle_path)
        if not lines:
            print("âŒ æœªè§£æåˆ°æœ‰æ•ˆå­—å¹•å†…å®¹")
            return []
        
        total = len(lines)
        if total <= 0:
            return []

        # å½“æ‰¹å¤„ç†å¤§å°å¤§äºå­—å¹•æ€»è¡Œæ•°æ—¶ï¼Œè‡ªåŠ¨æ”¶ç¼©åˆ°æ€»è¡Œæ•°
        if batch_size > total:
            batch_size = total
        if batch_size < 1:
            batch_size = 1
        
        # ===== æ–­ç‚¹æ¢å¤ï¼šåŠ è½½å·²å®Œæˆçš„è¡Œ =====
        completed_indices: set = set()
        results: List[LineAnnotation] = [None] * total
        
        if resume_from_checkpoint:
            checkpoint = load_checkpoint(movie_id)
            if checkpoint and checkpoint.get("completed_indices"):
                completed_indices = set(checkpoint["completed_indices"])
                # ä»å·²æœ‰çš„annotated JSONåŠ è½½å·²å®Œæˆçš„ç»“æœ
                ann_path = _annotation_output_path(movie_id)
                if ann_path.exists():
                    try:
                        with open(ann_path, "r", encoding="utf-8") as f:
                            existing_data = json.load(f)
                        for ann_dict in existing_data:
                            # ä»idä¸­æå–è¡Œå·ï¼ˆæ ¼å¼: {media_id}_line_{idx}ï¼‰
                            ann_id = ann_dict.get("id", "")
                            parts = ann_id.rsplit("_line_", 1)
                            if len(parts) == 2 and parts[1].isdigit():
                                idx = int(parts[1])
                                if 0 <= idx < total:
                                    results[idx] = LineAnnotation.from_dict(ann_dict)
                        print(f"ğŸ”„ ä»checkpointæ¢å¤: å·²å®Œæˆ {len(completed_indices)}/{total} è¡Œ")
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½å·²æœ‰æ ‡æ³¨å¤±è´¥ï¼Œä»å¤´å¼€å§‹: {e}")
                        completed_indices = set()
                        results = [None] * total
        
        # åˆ¤æ–­ä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼è¿˜æ˜¯å•è¡Œæ¨¡å¼
        use_batch_mode = batch_size > 1
        
        remaining = total - len(completed_indices)
        if remaining <= 0:
            print(f"âœ… æ‰€æœ‰ {total} è¡Œå·²æ ‡æ³¨å®Œæˆï¼Œæ— éœ€ç»§ç»­")
            results = [r for r in results if r is not None]
            return results
        
        if use_batch_mode:
            print(f"ğŸ”§ ä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼: batch_size={batch_size}, max_workers={max_workers}, save_interval={save_interval}")
            print(f"ğŸ“Š æ‰¾åˆ° {total} è¡Œå­—å¹•ï¼Œå¾…å¤„ç† {remaining} è¡Œ")
        else:
            print(f"ğŸ”§ ä½¿ç”¨å•è¡Œæ¨¡å¼: window_size={window_size}, max_workers={max_workers}, save_interval={save_interval}")
            print(f"ğŸ“Š æ‰¾åˆ° {total} è¡Œå­—å¹•ï¼Œå¾…å¤„ç† {remaining} è¡Œ")
        
        start_time = time.time()
        completed = len(completed_indices)
        last_save_completed = completed  # ä¸Šæ¬¡å¢é‡ä¿å­˜æ—¶çš„å®Œæˆæ•°
        paused = False  # æ˜¯å¦è¢«æš‚åœ
        
        # ===== å¢é‡ä¿å­˜è¾…åŠ©å‡½æ•° =====
        def _incremental_save():
            """å¢é‡ä¿å­˜å½“å‰ç»“æœå’Œcheckpoint"""
            nonlocal last_save_completed
            ANNOTATION_DIR.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜å·²å®Œæˆçš„æ ‡æ³¨ç»“æœ
            completed_results = [r for r in results if r is not None]
            if completed_results:
                out_path = _annotation_output_path(movie_id)
                data = [a.to_dict() for a in completed_results]
                with open(out_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜checkpoint
            current_completed = [i for i in range(total) if results[i] is not None]
            cp_data = {
                "movie_id": movie_id,
                "movie_name": movie_name,
                "subtitle_path": subtitle_path,
                "llm_provider": self.provider_name,
                "total_lines": total,
                "completed_indices": current_completed,
                "completed_count": len(current_completed),
                "last_save_time": time.time(),
                "batch_size": batch_size,
                "save_interval": save_interval
            }
            cp_path = _checkpoint_path(movie_id)
            with open(cp_path, "w", encoding="utf-8") as f:
                json.dump(cp_data, f, ensure_ascii=False, indent=2)
            
            last_save_completed = completed
            print(f"ğŸ’¾ å¢é‡ä¿å­˜: {len(current_completed)}/{total} è¡Œ ({len(current_completed)/total:.1%})")
        
        def _check_pause():
            """æ£€æŸ¥æš‚åœäº‹ä»¶ï¼Œå¦‚æœè®¾ç½®äº†å°±ç­‰å¾…"""
            if pause_event and pause_event.is_set():
                print(f"â¸ï¸ æ ‡æ³¨å·²æš‚åœï¼Œå½“å‰è¿›åº¦ {completed}/{total}")
                _incremental_save()
                # ç­‰å¾…æš‚åœè§£é™¤æˆ–å–æ¶ˆ
                while pause_event.is_set():
                    if cancel_event and cancel_event.is_set():
                        return True  # æš‚åœæœŸé—´è¢«å–æ¶ˆ
                    time.sleep(0.5)
                print(f"â–¶ï¸ æ ‡æ³¨æ¢å¤")
            return False
        
        if use_batch_mode:
            # æ‰¹å¤„ç†æ¨¡å¼ï¼šå°†å°è¯åˆ†æ‰¹ï¼Œæ¯æ‰¹ä¸€æ¬¡LLMè°ƒç”¨
            batches = []
            for i in range(0, total, batch_size):
                if cancel_event and cancel_event.is_set():
                    break
                batch = []
                for j in range(i, min(i + batch_size, total)):
                    if j in completed_indices:
                        continue  # è·³è¿‡å·²å®Œæˆçš„è¡Œ
                    # é¢„è®¡ç®—ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæ‰¹é‡å¤±è´¥æ—¶çš„å›é€€å•è¡Œæ ‡æ³¨ï¼‰
                    start_idx = max(0, j - window_size)
                    end_idx = min(total, j + window_size + 1)
                    context = [lines[k]["text"] for k in range(start_idx, end_idx) if k != j]
                    batch.append({
                        "idx": j,
                        "text": lines[j]["text"],
                        "start": lines[j]["start"],
                        "end": lines[j]["end"],
                        "context": context
                    })
                if batch:
                    batches.append(batch)
            
            def process_batch(batch):
                return self.annotate_batch(batch, movie_name, actual_media_id, subtitle_path)
            
            # å¹¶è¡Œå¤„ç†å¤šä¸ªæ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {}
                for batch in batches:
                    if cancel_event and cancel_event.is_set():
                        break
                    if _check_pause():
                        break
                    futures[executor.submit(process_batch, batch)] = batch
                
                for future in as_completed(futures):
                    if cancel_event and cancel_event.is_set():
                        for f in futures:
                            f.cancel()
                        break
                    if _check_pause():
                        for f in futures:
                            f.cancel()
                        break
                    batch = futures[future]
                    try:
                        batch_results = future.result()
                        for idx, annotation in batch_results:
                            results[idx] = annotation
                            completed += 1
                            completed_indices.add(idx)
                            
                            if progress_callback and not (cancel_event and cancel_event.is_set()):
                                progress_callback(completed, total)
                        
                        # æ§åˆ¶å°è¿›åº¦
                        elapsed = time.time() - start_time
                        speed = completed / elapsed if elapsed > 0 else 0
                        print(f"ğŸ”„ è¿›åº¦: {completed}/{total} ({completed/total:.1%}) | é€Ÿåº¦: {speed:.1f}è¡Œ/ç§’")
                        
                        # ===== å¢é‡ä¿å­˜æ£€æŸ¥ =====
                        if save_interval > 0 and (completed - last_save_completed) >= save_interval:
                            _incremental_save()
                        
                    except Exception as e:
                        print(f"âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                        for item in batch:
                            idx = item["idx"]
                            results[idx] = LineAnnotation(
                                id=f"{actual_media_id}_line_{idx}",
                                text=item["text"],
                                source=SourceInfo(
                                    media_id=actual_media_id,
                                    start=item["start"],
                                    end=item["end"]
                                ),
                                annotated_at=time.time()
                            )
                            completed += 1
                            completed_indices.add(idx)
                            if progress_callback and not (cancel_event and cancel_event.is_set()):
                                progress_callback(completed, total)
        else:
            # å•è¡Œæ¨¡å¼ï¼šæ¯è¡Œå•ç‹¬ä¸€æ¬¡LLMè°ƒç”¨
            def process_line(idx: int) -> LineAnnotation:
                line = lines[idx]
                start_idx = max(0, idx - window_size)
                end_idx = min(total, idx + window_size + 1)
                context = [lines[j]["text"] for j in range(start_idx, end_idx) if j != idx]
                
                return self.annotate_line(
                    text=line["text"],
                    context_lines=context,
                    source_movie=actual_media_id,  # ä½¿ç”¨movie_idä½œä¸ºmedia_id
                    source_file=subtitle_path,
                    start=line["start"],
                    end=line["end"],
                    line_id=f"{actual_media_id}_line_{idx}"
                )
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {}
                for i in range(total):
                    if i in completed_indices:
                        continue  # è·³è¿‡å·²å®Œæˆçš„è¡Œ
                    if cancel_event and cancel_event.is_set():
                        break
                    if _check_pause():
                        break
                    futures[executor.submit(process_line, i)] = i
                
                for future in as_completed(futures):
                    if cancel_event and cancel_event.is_set():
                        for f in futures:
                            f.cancel()
                        break
                    if _check_pause():
                        for f in futures:
                            f.cancel()
                        break
                    idx = futures[future]
                    try:
                        results[idx] = future.result()
                        completed += 1
                        completed_indices.add(idx)
                        
                        if progress_callback and not (cancel_event and cancel_event.is_set()):
                            progress_callback(completed, total)
                        
                        if completed % max(1, total // 10) == 0:
                            elapsed = time.time() - start_time
                            speed = completed / elapsed if elapsed > 0 else 0
                            print(f"ğŸ”„ è¿›åº¦: {completed}/{total} ({completed/total:.1%}) | é€Ÿåº¦: {speed:.1f}è¡Œ/ç§’")
                        
                        # ===== å¢é‡ä¿å­˜æ£€æŸ¥ =====
                        if save_interval > 0 and (completed - last_save_completed) >= save_interval:
                            _incremental_save()
                            
                    except Exception as e:
                        print(f"âŒ è¡Œ {idx} å¤„ç†å¤±è´¥: {e}")
                        results[idx] = LineAnnotation(
                            id=f"{actual_media_id}_line_{idx}",
                            text=lines[idx]["text"],
                            source=SourceInfo(
                                media_id=actual_media_id,
                                start=lines[idx]["start"],
                                end=lines[idx]["end"]
                            ),
                            annotated_at=time.time()
                        )
                        completed += 1
                        completed_indices.add(idx)
        
        # æ£€æŸ¥æ˜¯å¦å› æš‚åœè€Œä¸­æ–­
        if pause_event and pause_event.is_set():
            _incremental_save()
            paused = True
            print(f"â¸ï¸ æ ‡æ³¨æš‚åœï¼Œå·²ä¿å­˜ {completed}/{total} è¡Œ")
            # è¿”å›å·²å®Œæˆçš„éƒ¨åˆ†
            results = [r for r in results if r is not None]
            return results
        
        # æ£€æŸ¥æ˜¯å¦å› å–æ¶ˆè€Œä¸­æ–­
        if cancel_event and cancel_event.is_set():
            # å–æ¶ˆæ—¶ä¹Ÿåšå¢é‡ä¿å­˜ï¼ˆä¿ç•™å·²å®Œæˆçš„éƒ¨åˆ†ï¼‰
            _incremental_save()
            print(f"âš ï¸ æ ‡æ³¨å·²å–æ¶ˆï¼Œå·²ä¿å­˜ {completed}/{total} è¡Œ")
            results = [r for r in results if r is not None]
            return results
        
        # è¿‡æ»¤None
        results = [r for r in results if r is not None]
        
        # æ­£å¸¸å®Œæˆï¼šåšæœ€ç»ˆä¿å­˜å¹¶åˆ é™¤checkpoint
        _incremental_save()
        delete_checkpoint(movie_id)
        
        print(f"âœ… æ ‡æ³¨å®Œæˆï¼å…± {len(results)} è¡Œï¼Œè€—æ—¶ {time.time() - start_time:.1f}ç§’")
        
        return results
    
    def save_annotations(
        self, 
        annotations: List[LineAnnotation], 
        output_path: str
    ):
        """ä¿å­˜æ ‡æ³¨ç»“æœ"""
        data = [a.to_dict() for a in annotations]
        
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ ‡æ³¨ç»“æœå·²ä¿å­˜: {output_path}")


# ==================== CLI ====================
def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="å°è¯æ··å‰ªè¯­ä¹‰æ ‡æ³¨å·¥å…· v5.0")
    parser.add_argument("input", help="SRTå­—å¹•æ–‡ä»¶è·¯å¾„")
    parser.add_argument("output", help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--movie", default="", help="ç”µå½±åç§°")
    parser.add_argument("--provider", default=None, help="LLMæä¾›è€… (local_qwen, openai, deepseekç­‰)")
    parser.add_argument("--window", type=int, default=2, help="ä¸Šä¸‹æ–‡çª—å£å¤§å°")
    parser.add_argument("--workers", type=int, default=4, help="å¹¶å‘çº¿ç¨‹æ•°")
    parser.add_argument("--list-providers", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„LLMæä¾›è€…")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæä¾›è€…
    if args.list_providers:
        manager = LLMProviderManager()
        print("\nğŸ“‹ å¯ç”¨çš„LLMæä¾›è€…:")
        for p in manager.list_providers():
            status = "âœ… å½“å‰" if p["is_active"] else "  "
            print(f"  {status} {p['id']}: {p['name']} ({p['type']})")
            if p["description"]:
                print(f"       {p['description']}")
        return
    
    # æ‰§è¡Œæ ‡æ³¨
    print("=" * 60)
    print("ğŸ¬ å°è¯æ··å‰ªè¯­ä¹‰æ ‡æ³¨å·¥å…· v5.0")
    print("=" * 60)
    
    annotator = SemanticAnnotator(llm_provider=args.provider)
    
    annotations = annotator.annotate_subtitle_file(
        subtitle_path=args.input,
        movie_name=args.movie or Path(args.input).stem,
        window_size=args.window,
        max_workers=args.workers
    )
    
    if annotations:
        annotator.save_annotations(annotations, args.output)
        
        # æ˜¾ç¤ºç¤ºä¾‹
        print("\nğŸ“ æ ‡æ³¨ç¤ºä¾‹:")
        for ann in annotations[:3]:
            tags = ann.mashup_tags
            print(f"  å°è¯: {ann.text[:40]}...")
            print(f"  å¥å‹: {tags.sentence_type} | æƒ…ç»ª: {tags.emotion} | è¯­æ°”: {tags.tone}")
            print(f"  å¯æ¥: {tags.can_follow} | å¯å¼•: {tags.can_lead_to}")
            print(f"  å‘é‡æ–‡æœ¬: {ann.vector_text[:60]}...")
            print()


if __name__ == "__main__":
    main()
